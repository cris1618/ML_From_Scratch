#+TITLE: Support Vector Machine from Scratch
#+AUTHOR: Cristian Del Gobbo (pledged)
#+STARTUP: overview hideblocks indent
#+property: header-args:python :python python3 :session *Python* :results output :exports both :noweb yes :tangle yes:

* Introduction
In this notebook, I will implement a Support Vector Machine (SVM) algorithm 
from scratch using Python and C, without relying on any ML related external libraries.
* Algorithm Description
* Code
** Python Code
Let's start by importing the data. The dataset used is "The Wisconsin Breast Cancer dataset (WDBC)".
This dataset consists of 569 samples of breast cancer cell nuclei. Each sample is described by 30 
numerical features derived from digitized images of fine needle aspirates (FNA). The diagnosis column 
indicates whether the tumor is malignant (M) or benign (B). This dataset is commonly used for 
binary classification tasks.

The dataset includes:
- 1 ID column (sample identifier)
- 1 Diagnosis column (M = malignant, B = benign)
- 30 Features describing the cell nuclei's properties (radius, texture, perimeter, area, etc.)

The goal is to predict the diagnosis based on the input features. 
#+name: data
#+begin_src python :python python3 :results output
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

dir = os.getcwdb().decode('utf-8')
file_path = os.path.join(dir, "Datasets/breast_cancer/wdbc.data")

# Columns names
columns = [
    "ID", "Diagnosis", "Radius_mean", "Texture_mean", "Perimeter_mean", "Area_mean", "Smoothness_mean", 
    "Compactness_mean", "Concavity_mean", "Concave_points_mean", "Symmetry_mean", "Fractal_dimension_mean",
    "Radius_se", "Texture_se", "Perimeter_se", "Area_se", "Smoothness_se", "Compactness_se", "Concavity_se", 
    "Concave_points_se", "Symmetry_se", "Fractal_dimension_se",
    "Radius_worst", "Texture_worst", "Perimeter_worst", "Area_worst", "Smoothness_worst", 
    "Compactness_worst", "Concavity_worst", "Concave_points_worst", "Symmetry_worst", "Fractal_dimension_worst"
]

# Import the data
data = pd.read_csv(file_path, sep=",", header=None, names=columns)
data["Diagnosis"] = data["Diagnosis"].replace("M", -1)
data["Diagnosis"] = data["Diagnosis"].replace("B", 1)

X = data.drop(["Diagnosis", "ID"], axis=1)
y = data["Diagnosis"]

#print(data.head())
#+end_src

#+RESULTS: data
#+begin_example
/tmp/babel-4Pwgpn/python-UWWTW6:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  data["Diagnosis"] = data["Diagnosis"].replace("B", 0)
   Radius_mean  Texture_mean  ...  Symmetry_worst  Fractal_dimension_worst
0        17.99         10.38  ...          0.4601                  0.11890
1        20.57         17.77  ...          0.2750                  0.08902
2        19.69         21.25  ...          0.3613                  0.08758
3        11.42         20.38  ...          0.6638                  0.17300
4        20.29         14.34  ...          0.2364                  0.07678

[5 rows x 30 columns]
#+end_example

Since SVMs are sensitive to feature scaling, it's important
to standardize the data before passing them to the model.
#+name: preprocess
#+begin_src python :python python3 :results output
<<data>>
# Create function to split the data (similar to scikit-learn train_test_split)
def train_test_split(X, y, test_size=0.2, random_state=None):
    if random_state:
        np.random.seed(random_state)
    
    # Shuffle data
    indices = np.arange(X.shape[0])
    np.random.shuffle(indices)
    
    X_shuffled = X.iloc[indices]
    y_shuffled = y.iloc[indices]
   
    split_index = int(X.shape[0] * (1 - test_size))

    X_train, X_test = X_shuffled[:split_index], X_shuffled[split_index:]
    y_train, y_test = y_shuffled[:split_index], y_shuffled[split_index:]
    
    return X_train, X_test, y_train, y_test

# Create a custom Standard Scaler Class (To replicate the scikit-learn class "StandardScaler")
class StandardScaler:
    def __init__(self):
        self.mean = None
        self.std = None
    
    def fit(self, X):
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)

    def transform(self, X):
        return (X - self.mean) / self.std
    
    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)

# Apply to the dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, 0.2, random_state=1618)

# Ensure they are numerical
X_train = np.array(X_train.astype(float))
y_train = np.array(y_train.astype(int))
#+end_src

Unlike the other Alghoritms I implemented in this "ML from Scratch" repository,
for the SVM I'll use a single class for training and predicting, instead of
using separate functions for loss calculation, gradient descent and then predictions.
#+name: svm
#+begin_src python :python python3 :results output
<<preprocess>>
# SVM class with Kernel trick
class SVM:
    def __init__(self, learning_rate=0.001, lambda_param=0.01, num_epochs=1000, kernel="linear", degree=3, gamma=0.1):
        self.learning_rate = learning_rate
        self.lambda_param = lambda_param
        self.num_epochs = num_epochs
        self.kernel = kernel
        self.degree = degree
        self.gamma = gamma
        self.weights = None
        self.bias = 0
        self.losses = []
        self.X_train = None
    
    # Define Kernel functions
    def linear_kernel(self, x1, x2):
        return np.dot(x1, x2)

    def polynomial_kernel(self, x1, x2):
        return (np.dot(x1, x2) + 1) ** self.degree

    def rbf_kernel(self, x1, x2):
        return np.exp(-self.gamma * np.linalg.norm(x1 - x2) ** 2)

    # Apply kernel
    def apply_kernel(self, X, Y=None):
        if Y is None:
            Y = X
        n_samples = X.shape[0]
        m_samples = Y.shape[0]
        K = np.zeros((n_samples, m_samples))
        for i in range(n_samples):
            for j in range(m_samples):
                if self.kernel == "linear":
                    K[i, j] = self.linear_kernel(X[i], Y[j])
                elif self.kernel == "poly":
                    K[i, j] = self.polynomial_kernel(X[i], Y[j])
                elif self.kernel == "rbf":
                    K[i, j] = self.rbf_kernel(X[i], Y[j])

        return K 

    # Hinge loss definition
    def hinge_loss(self, X, y):
        n_samples = X.shape[0]
        distances = 1 - y * (np.dot(X, self.weights) - self.bias)
        distances = np.maximum(0, distances)
        hinge_loss = self.lambda_param * np.dot(self.weights, self.weights) + np.mean(distances)
        return hinge_loss
    
    # Training :)
    def train(self, X, y):
        n_samples, n_features = X.shape
        self.X_train = X
        if self.kernel == "linear":
            self.weights = np.zeros(n_features)
        else:
            self.weights = np.zeros(n_samples) # for non-linear kernels

        # Apply kernel (if necessary)
        if self.kernel != "linear":
            X = self.apply_kernel(X)
        
        # Converting labels to -1 and 1
        y_ = np.where(y <= 0, -1, 1)
       
        for epoch in range(self.num_epochs):
            for i, x_i in enumerate(X):
                condition = (y_[i] * (np.dot(x_i, self.weights) - self.bias)) >= 1
                if condition:
                    self.weights -= self.learning_rate * (2 * self.lambda_param * self.weights)
                else:
                    self.weights -= self.learning_rate * (2 * self.lambda_param * self.weights - np.dot(x_i, y_[i]))
                    self.bias -= self.learning_rate * y_[i]

            # Track loss at each epoch
            loss = self.hinge_loss(X, y_)
            self.losses.append(loss)
            if epoch % 100 == 0:
                print(f"Epoch: {epoch}, Loss: {loss:.4f}")

    def predict(self, X):
        if isinstance(X, pd.DataFrame):
            X = X.to_numpy()
        if self.kernel != "linear":
            X = self.apply_kernel(X, self.X_train) # Kernel between test and train
        approx = np.dot(X, self.weights) - self.bias
        return np.sign(approx)

    def evaluate(self, X, y):
        y_pred = self.predict(X)
        accuracy = np.mean(y_pred == np.where(y <= 0, -1, 1))
        print(f"Model Accuracy: {accuracy * 100:.2f}%")
        return accuracy
  #+end_src

Now,let's test the model!
#+name: test
#+begin_src python :python python3 :results output
<<svm>>
# Model initialization
svm_classifier = SVM(learning_rate=0.001, num_epochs=1000, kernel="linear")

# Train the model
svm_classifier.train(X_train, y_train) 

# Test the model
y_pred = svm_classifier.predict(X_test)

# Evaluate the model
svm_classifier.evaluate(X_test, y_test)
#+end_src

#+RESULTS: test
#+begin_example
/tmp/babel-vtABmf/python-ZDageF:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  data["Diagnosis"] = data["Diagnosis"].replace("B", 1)
Epoch: 0, Loss: 0.1850
Epoch: 100, Loss: 0.0814
Epoch: 200, Loss: 0.0809
Epoch: 300, Loss: 0.0809
Epoch: 400, Loss: 0.0809
Epoch: 500, Loss: 0.0810
Epoch: 600, Loss: 0.0809
Epoch: 700, Loss: 0.0809
Epoch: 800, Loss: 0.0809
Epoch: 900, Loss: 0.0809
Model Accuracy: 97.37%
#+end_example

** C Code

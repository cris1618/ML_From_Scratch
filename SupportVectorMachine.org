#+TITLE: Support Vector Machine from Scratch
#+AUTHOR: Cristian Del Gobbo (pledged)
#+STARTUP: overview hideblocks indent
#+property: header-args:python :python python3 :session *Python* :results output :exports both :noweb yes :tangle yes:

* Introduction
In this notebook, I will implement a Support Vector Machine (SVM) algorithm 
from scratch using Python and C, without relying on any ML related external libraries.
* Algorithm Description
The Support Vector Machine (SVM) is a supervised learning algorithm used for binary classification tasks, 
and other tasks such as multi-class classifictaion and regression. It aims to find the optimal hyperplane 
that maximizes the margin between two classes in feature space.

=Key Steps=

1. *Data Preprocessing*
   - Scale features using =standardization= to ensure all dimensions have equal importance.
   - Convert labels to \(-1\) and \(1\) for hinge loss compatibility.

2. *Kernel Trick*
   - If the data is not linearly separable, use kernel functions to map it to a higher-dimensional space:
     - *Linear Kernel*: \( K(x, x') = x \cdot x' \)
     - *Polynomial Kernel*: \( K(x, x') = (x \cdot x' + 1)^d \)
     - *RBF Kernel*: \( K(x, x') = \exp(-\gamma \|x - x'\|^2) \)

3. *Loss Function*
   - Minimize the =hinge loss= to find the best separating hyperplane:
     \[
     L(w, b) = \lambda \|w\|^2 + \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i (w \cdot x_i - b))
     \]
   - \( w \): Weights
   - \( b \): Bias
   - \( \lambda \): Regularization parameter
   - \( x_i, y_i \): Features and labels of the \(i\)-th sample.

4. *Training*
   - Optimize \(w\) and \(b\) using gradient descent or similar optimization techniques.
   - Adjust \(w\) and \(b\) iteratively to minimize the loss function.

5. *Prediction*
   - Predict the class of a sample \(x\) as:
     \[
     \hat{y} = \text{sign}(w \cdot x - b)
     \]

Now, Let's code!
* Code
** Python Code
Let's start by importing the data. The dataset used is "The Wisconsin Breast Cancer dataset (WDBC)".
This dataset consists of 569 samples of breast cancer cell nuclei. Each sample is described by 30 
numerical features derived from digitized images of fine needle aspirates (FNA). The diagnosis column 
indicates whether the tumor is malignant (M) or benign (B). This dataset is commonly used for 
binary classification tasks.

The dataset includes:
- 1 ID column (sample identifier)
- 1 Diagnosis column (M = malignant, B = benign)
- 30 Features describing the cell nuclei's properties (radius, texture, perimeter, area, etc.)

The goal is to predict the diagnosis based on the input features. 
#+name: data
#+begin_src python :python python3 :results output
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import os

  dir = os.getcwdb().decode('utf-8')
  file_path = os.path.join(dir, "Datasets/breast_cancer/wdbc.data")

  # Columns names
  columns = [
      "ID", "Diagnosis", "Radius_mean", "Texture_mean", "Perimeter_mean", "Area_mean", "Smoothness_mean", 
      "Compactness_mean", "Concavity_mean", "Concave_points_mean", "Symmetry_mean", "Fractal_dimension_mean",
      "Radius_se", "Texture_se", "Perimeter_se", "Area_se", "Smoothness_se", "Compactness_se", "Concavity_se", 
      "Concave_points_se", "Symmetry_se", "Fractal_dimension_se",
      "Radius_worst", "Texture_worst", "Perimeter_worst", "Area_worst", "Smoothness_worst", 
      "Compactness_worst", "Concavity_worst", "Concave_points_worst", "Symmetry_worst", "Fractal_dimension_worst"
  ]

  # Import the data
  data = pd.read_csv(file_path, sep=",", header=None, names=columns)
  data["Diagnosis"] = data["Diagnosis"].replace("M", -1)
  data["Diagnosis"] = data["Diagnosis"].replace("B", 1)

  X = data.drop(["Diagnosis", "ID"], axis=1)
  y = data["Diagnosis"]

  #print(data.head())
#+end_src

#+RESULTS: data
#+begin_example
/tmp/babel-4Pwgpn/python-UWWTW6:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  data["Diagnosis"] = data["Diagnosis"].replace("B", 0)
   Radius_mean  Texture_mean  ...  Symmetry_worst  Fractal_dimension_worst
0        17.99         10.38  ...          0.4601                  0.11890
1        20.57         17.77  ...          0.2750                  0.08902
2        19.69         21.25  ...          0.3613                  0.08758
3        11.42         20.38  ...          0.6638                  0.17300
4        20.29         14.34  ...          0.2364                  0.07678

[5 rows x 30 columns]
#+end_example

Since SVMs are sensitive to feature scaling, it's important
to standardize the data before passing them to the model.
#+name: preprocess
#+begin_src python :python python3 :results output
  <<data>>
  # Create function to split the data (similar to scikit-learn train_test_split)
  def train_test_split(X, y, test_size=0.2, random_state=None):
      if random_state:
          np.random.seed(random_state)

      # Shuffle data
      indices = np.arange(X.shape[0])
      np.random.shuffle(indices)

      X_shuffled = X.iloc[indices]
      y_shuffled = y.iloc[indices]

      split_index = int(X.shape[0] * (1 - test_size))

      X_train, X_test = X_shuffled[:split_index], X_shuffled[split_index:]
      y_train, y_test = y_shuffled[:split_index], y_shuffled[split_index:]

      return X_train, X_test, y_train, y_test

  # Create a custom Standard Scaler Class (To replicate the scikit-learn class "StandardScaler")
  class StandardScaler:
      def __init__(self):
          self.mean = None
          self.std = None

      def fit(self, X):
          self.mean = np.mean(X, axis=0)
          self.std = np.std(X, axis=0)

      def transform(self, X):
          return (X - self.mean) / self.std

      def fit_transform(self, X):
          self.fit(X)
          return self.transform(X)

  # Apply to the dataset
  scaler = StandardScaler()
  X_scaled = scaler.fit_transform(X)

  # Split the data
  X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, 0.2, random_state=1618)

  # Ensure they are numerical
  X_train = np.array(X_train.astype(float))
  y_train = np.array(y_train.astype(int))
  #+end_src

Unlike the other Alghoritms I implemented in this "ML from Scratch" repository,
for the SVM I'll use a single class for training and predicting, instead of
using separate functions for loss calculation, gradient descent and then predictions.
#+name: svm
#+begin_src python :python python3 :results output
  <<preprocess>>
  # SVM class with Kernel trick
  class SVM:
      def __init__(self, learning_rate=0.001, lambda_param=0.01, num_epochs=1000, kernel="linear", degree=3, gamma=0.1):
          self.learning_rate = learning_rate
          self.lambda_param = lambda_param
          self.num_epochs = num_epochs
          self.kernel = kernel
          self.degree = degree
          self.gamma = gamma
          self.weights = None
          self.bias = 0
          self.losses = []
          self.X_train = None

      # Define Kernel functions
      def linear_kernel(self, x1, x2):
          return np.dot(x1, x2)

      def polynomial_kernel(self, x1, x2):
          return (np.dot(x1, x2) + 1) ** self.degree

      def rbf_kernel(self, x1, x2):
          return np.exp(-self.gamma * np.linalg.norm(x1 - x2) ** 2)

      # Apply kernel
      def apply_kernel(self, X, Y=None):
          if Y is None:
              Y = X
              n_samples = X.shape[0]
              m_samples = Y.shape[0]
              K = np.zeros((n_samples, m_samples))
          for i in range(n_samples):
              for j in range(m_samples):
                  if self.kernel == "linear":
                      K[i, j] = self.linear_kernel(X[i], Y[j])
                  elif self.kernel == "poly":
                      K[i, j] = self.polynomial_kernel(X[i], Y[j])
                  elif self.kernel == "rbf":
                      K[i, j] = self.rbf_kernel(X[i], Y[j])

          return K 

      # Hinge loss definition
      def hinge_loss(self, X, y):
          n_samples = X.shape[0]
          distances = 1 - y * (np.dot(X, self.weights) - self.bias)
          distances = np.maximum(0, distances)
          hinge_loss = self.lambda_param * np.dot(self.weights, self.weights) + np.mean(distances)
          return hinge_loss

      # Training :)
      def train(self, X, y):
          n_samples, n_features = X.shape
          self.X_train = X
          if self.kernel == "linear":
              self.weights = np.zeros(n_features)
          else:
              self.weights = np.zeros(n_samples) # for non-linear kernels

          # Apply kernel (if necessary)
          if self.kernel != "linear":
              X = self.apply_kernel(X)

          # Converting labels to -1 and 1
          y_ = np.where(y <= 0, -1, 1)

          for epoch in range(self.num_epochs):
              for i, x_i in enumerate(X):
                  condition = (y_[i] * (np.dot(x_i, self.weights) - self.bias)) >= 1
                  if condition:
                      self.weights -= self.learning_rate * (2 * self.lambda_param * self.weights)
                  else:
                      self.weights -= self.learning_rate * (2 * self.lambda_param * self.weights - np.dot(x_i, y_[i]))
                      self.bias -= self.learning_rate * y_[i]

              # Track loss at each epoch
              loss = self.hinge_loss(X, y_)
              self.losses.append(loss)
              if epoch % 100 == 0:
                  print(f"Epoch: {epoch}, Loss: {loss:.4f}")

      def predict(self, X):
          if isinstance(X, pd.DataFrame):
              X = X.to_numpy()
          if self.kernel != "linear":
              X = self.apply_kernel(X, self.X_train) # Kernel between test and train
              approx = np.dot(X, self.weights) - self.bias
          return np.sign(approx)

      def evaluate(self, X, y):
          y_pred = self.predict(X)
          accuracy = np.mean(y_pred == np.where(y <= 0, -1, 1))
          print(f"Model Accuracy: {accuracy * 100:.2f}%")
          return accuracy
#+end_src

Now,let's test the model!
#+name: test
#+begin_src python :python python3 :results output
  <<svm>>
  # Model initialization
  svm_classifier = SVM(learning_rate=0.001, num_epochs=1000, kernel="linear")

  # Train the model
  svm_classifier.train(X_train, y_train) 

  # Test the model
  y_pred = svm_classifier.predict(X_test)

  # Evaluate the model
  svm_classifier.evaluate(X_test, y_test)
#+end_src

#+RESULTS: test
#+begin_example
/tmp/babel-vtABmf/python-ZDageF:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  data["Diagnosis"] = data["Diagnosis"].replace("B", 1)
Epoch: 0, Loss: 0.1850
Epoch: 100, Loss: 0.0814
Epoch: 200, Loss: 0.0809
Epoch: 300, Loss: 0.0809
Epoch: 400, Loss: 0.0809
Epoch: 500, Loss: 0.0810
Epoch: 600, Loss: 0.0809
Epoch: 700, Loss: 0.0809
Epoch: 800, Loss: 0.0809
Epoch: 900, Loss: 0.0809
Model Accuracy: 97.37%
#+end_example

** C Code
As usual, let's create the same SVM model in C.
1) Import the data
#+name: import_data
#+begin_src C :main no :results output :noweb yes
  #include <stdio.h>
  #include <stdlib.h>
  #include <string.h>
  #include <ctype.h>
  #include <math.h>

  // Define dimensions
  #define MAX_FEATURES 30
  #define MAX_SAMPLES 600

  // Dataset structure
  typedef struct Sample{
    double features[MAX_FEATURES];
    int label; 
  } Sample;

  // Standard Scaler structure
  typedef struct StandardScaler{
    double* mean;
    double* std;
    int num_features;
  } StandardScaler;

  // Name: load_data
  // Purpose: Load a dataset csv file.
  // Return: int, number of line 
  // Arguments: Filename, Struct to store data, max number of samples.
  int load_data(const char* filename, Sample* dataset, int max_samples){
    FILE* file = fopen(filename, "r");
    if(!file){
      perror("Failed to open file");
      return -1;
    }

    char line[1024];
    int sample_count = 0;

    while(fgets(line, sizeof(line), file)){
      if(sample_count >= max_samples){
        printf("Maximum sample limit reached.\n");
        break;
      }

      // Parse ID (ignore) and label
      char* token = strtok(line, ",");
      token = strtok(NULL, ","); // Skip ID

      // Convert "M" and "B" to -1 and 1
      if(strcmp(token, "M") == 0){
        dataset[sample_count].label = -1;
      } else if(strcmp(token, "B") == 0){
        dataset[sample_count].label = 1;
      } else {
        printf("Invalid label at line %d\n", sample_count + 1);
        fclose(file);
        return -1;
      }

      // Parse features
      int feature_index = 0;
      while((token = strtok(NULL, ",")) != NULL && feature_index < MAX_FEATURES){
        dataset[sample_count].features[feature_index++] = atof(token);
      }

      if(feature_index != MAX_FEATURES){
        printf("Incomplete features at line %d\n", sample_count + 1);
        fclose(file);
        return -1;
      }

      sample_count++;
    }

    fclose(file);
    return sample_count;
  }
  
  // Test the function
  /*int main(){
    Sample dataset[MAX_SAMPLES];
    int total_samples = load_data("wdbc.data", dataset, MAX_SAMPLES);

    if(total_samples > 0){
    printf("Loaded %d samples.\n", total_samples);

    for(int i = 0; i < 5 && i < total_samples; i++){
    printf("Sample %d:\n", i+1);
    printf("Label: %d\n", dataset[i].label);
    printf("Features: ");
    for(int j = 0; j < MAX_FEATURES; j++){
    printf("%.2f ", dataset[i].features[j]);
    }
    printf("\n");
    }
    }
    return 0;
    }*/
#+end_src

#+RESULTS: import_data
#+begin_example
Loaded 569 samples.
Sample 1:
Label: -1
Features: 17.99 10.38 122.80 1001.00 0.12 0.28 0.30 0.15 0.24 0.08 1.09 0.91 8.59 153.40 0.01 0.05 0.05 0.02 0.03 0.01 25.38 17.33 184.60 2019.00 0.16 0.67 0.71 0.27 0.46 0.12 
Sample 2:
Label: -1
Features: 20.57 17.77 132.90 1326.00 0.08 0.08 0.09 0.07 0.18 0.06 0.54 0.73 3.40 74.08 0.01 0.01 0.02 0.01 0.01 0.00 24.99 23.41 158.80 1956.00 0.12 0.19 0.24 0.19 0.28 0.09 
Sample 3:
Label: -1
Features: 19.69 21.25 130.00 1203.00 0.11 0.16 0.20 0.13 0.21 0.06 0.75 0.79 4.58 94.03 0.01 0.04 0.04 0.02 0.02 0.00 23.57 25.53 152.50 1709.00 0.14 0.42 0.45 0.24 0.36 0.09 
Sample 4:
Label: -1
Features: 11.42 20.38 77.58 386.10 0.14 0.28 0.24 0.11 0.26 0.10 0.50 1.16 3.44 27.23 0.01 0.07 0.06 0.02 0.06 0.01 14.91 26.50 98.87 567.70 0.21 0.87 0.69 0.26 0.66 0.17 
Sample 5:
Label: -1
Features: 20.29 14.34 135.10 1297.00 0.10 0.13 0.20 0.10 0.18 0.06 0.76 0.78 5.44 94.44 0.01 0.02 0.06 0.02 0.02 0.01 22.54 16.67 152.20 1575.00 0.14 0.20 0.40 0.16 0.24 0.08
#+end_example

2) Preprocess the data and define some helper functions.
#+name: preprocess_data
#+begin_src C :main no :results output :noweb yes
  <<import_data>>
    // Name: shuffle
    // Purpose: Shuffle indices.
    // Return: void
    // Arguments: indices,
    //            Number of rows (total number of indices to shuffle),
    //            Random State.
  void shuffle(int* indices, int num_rows, int random_state){
    srand(random_state);
    for(int i = num_rows - 1; i > 0; i--){
      int j = rand() % (i+1);
      int temp = indices[i];
      indices[i] = indices[j];
      indices[j] = temp;
    }
    }

  // Name: train_test_split
  // Purpose: Split the data for training and for testing.
  // Return: void
  // Arguments: X to split,
  //            y to split,
  //            X_train, X_test, y_train, y_test (Outputs),
  //            Number of rows (Samples),
  //            Number of features,
  //            Test size,
  //            Random State.
  void train_test_split(double* X, double* y, double* X_train, double* X_test, double* y_train, double* y_test,
                        int num_rows, int num_features, double test_size, int random_state){
 
    int indices[num_rows];
    for(int i = 0; i < num_rows; i++){
      indices[i] = i;
    }

    // Shuffle the indices
    srand(random_state);
    shuffle(indices, num_rows, 1618);

    int split_index = (int)(num_rows * (1 - test_size));

    // Split the data
    for(int i = 0; i<split_index; i++){
      int idx = indices[i];
      for(int j = 0; j<num_features; j++){
        X_train[i * num_features + j] = X[idx * num_features + j];
      }
      y_train[i] = y[idx];
    }

    for(int i = split_index; i<num_rows; i++){
      int idx = indices[i];
      for(int j = 0; j<num_features; j++){
        X_test[(i - split_index) * num_features + j] = X[idx * num_features + j];
      }
      y_test[i - split_index] = y[idx];
    }
  } 
  

  // Name: compute_mean_std
  // Purpose: Find the mean and standard deviation.
  // Return: void
  // Arguments: input data X,
  //            number of rows,
  //            number of features,
  //            mean array
  //            std array
  void compute_mean_std(const double* X, int num_rows, int num_features, double* mean, double* std){
    // Compute mean
    for(int j = 0; j<num_features; j++){
      mean[j] = 0.0;
      for(int i = 0; i<num_rows; i++){
        mean[j] += X[i * num_features + j];
      }
      mean[j] /= num_rows;
    }

    // Compute standard deviation
    for(int j = 0; j<num_features; j++){
      std[j] = 0.0;
      for(int i = 0; i<num_rows; i++){
        double diff = X[i * num_features + j] - mean[j];
        std[j] += diff * diff;
      }
      std[j] = sqrt(std[j] / num_rows);
    }
  }

  // Name: scaler_fit
  // Purpose: Fit the scaler (comp mean and std).
  // Return: void
  // Arguments: scaler struct, 
  //            input data X, 
  //            number of rows,
  //            number of features
  void scaler_fit(StandardScaler* scaler, const double* X, int num_rows, int num_features){
    scaler->mean = (double*)malloc(num_features * sizeof(double));
    scaler->std = (double*)malloc(num_features * sizeof(double));
    scaler->num_features = num_features;

    compute_mean_std(X, num_rows, num_features, scaler->mean, scaler->std);
  }

  // Name: scaler_transform
  // Purpose: Transform the data.
  // Return: void
  // Arguments: scaler struct, 
  //            input data X, 
  //            number of rows
  void scaler_transform(const StandardScaler* scaler, double* X, int num_rows){
    for(int i = 0; i<num_rows; i++){
      for(int j = 0; j<scaler->num_features; j++){
        X[i * scaler->num_features + j] = (X[i * scaler->num_features + j] - scaler->mean[j]) / scaler->std[j];
      }
    }
  }
 
  // Name: scaler_fit_transform
  // Purpose: Fit and transform the data in one step.
  // Return: void
  // Arguments: scaler struct, 
  //            input data X, 
  //            number of rows,
  //            number of features
  void scaler_fit_transform(StandardScaler* scaler, double* X, int num_rows, int num_features){
    scaler_fit(scaler, X, num_rows, num_features);
    scaler_transform(scaler, X, num_rows);
  }

  // Name: scaler_free
  // Purpose: Free the allocated memory.
  // Return: void
  // Arguments: scaler struct.
  void scalar_free(StandardScaler* scaler){
    free(scaler->mean);
    free(scaler->std);
  }
#+end_src

#+RESULTS: preprocess_data

3) Create remaining helper functions (Similar to NumPy in Python)
#+name: math
#+begin_src C :main no :results output :noweb yes
  <<preprocess_data>>
    // Name: dot_product
    // Purpose: Compute the dot product of two vectors.
    // Return: double
    // Arguments: vector 1,
    //            vector 2,
    //            size of the vectors
  double dot_product(const double* vec1, const double* vec2, int size){
    double result = 0.0f;
    for(int i = 0; i<size; i++){
      result += vec1[i] * vec2[i];
    }
    return result;
    }

  // Name: euclidean_norm
  // Purpose: Compute Eucledian norm.
  // Return: double
  // Arguments: vector,
  //            size of the vector
  double euclidean_norm(const double* vec, int size){
    double sum = 0.0f;
    for(int i = 0; i<size; i++){
      sum += vec[i] * vec[i];
    }
    return sqrt(sum);
  }

  // Name: power
  // Purpose: Exponentiate.
  // Return: double
  // Arguments: base,
  //            exponent,
  double power(double base, int exponent){
    double result = 1.0f;
    for(int i = 0; i<exponent; i++){
      result *= base;
    }
    return result;
  }

  // Name: sign
  // Purpose: Sign function
  // Return: int
  // Arguments: value.
  int sign(double value){
    if(value > 0) return 1;
    else if (value < 0) return -1;
    return 0;
  }

  // Name: max
  // Purpose: Return maximum of two values.
  // Return: double
  // Arguments: value 1,
  //            value 2,
  double max(double a, double b){
    return (a > b) ? a : b;
  }
    #+end_src

#+RESULTS: math

4) SVM algorithm
#+name: svm_C
#+begin_src C :main no :results output :noweb yes
  <<math>>
    // Kernel functions

    // Name: linear_kernel
    // Purpose: Apply linear kernel.
    // Return: double
    // Arguments: vector 1,
    //            vector 2,
    //            size of the vectors
  double linear_kernel(const double* vec1, const double* vec2, int size){
    return dot_product(vec1, vec2, size);
    }

  // Name: polynomial_kernel
  // Purpose: Apply polynomial kernel.
  // Return: double
  // Arguments: vector 1,
  //            vector 2,
  //            size of the vectors,
  //            degree
  double polynomial_kernel(const double* vec1, const double* vec2, int size, int degree){
    double dot = dot_product(vec1, vec2, size);
    return power(dot + 1, degree);
  }

  // Name: rbf_kernel
  // Purpose: Apply Gaussian kernel.
  // Return: double
  // Arguments: vector 1,
  //            vector 2,
  //            size of the vectors,
  //            gamma parameter
  double rbf_kernel(const double* vec1, const double* vec2, int size, double gamma){
    double norm_diff = 0.0f;
    for(int i = 0; i<size; i++){
      double diff = vec1[i] - vec2[i];
      norm_diff += diff * diff;
    }
    return exp(-gamma * norm_diff);
  }

  // Name: hinge_loss
  // Purpose: Compute the hinge loss function.
  // Return: double
  // Arguments: weights,
  //            bias,
  //            input data X
  //            target y
  //            number of samples (rows)
  //            number of features
  //            lambda parameter
  double hinge_loss(const double* weights, double bias, const double* X, const double* y, int num_samples, int num_features, double lambda_param){
    double loss = 0.0;

    for(int i = 0; i<num_samples; i++){
      double margin = y[i] * (dot_product(weights, &X[i * num_features], num_features) - bias);
      loss += max(0, 1-margin);
    }

    // Add regularization term
    double regularization = 0.0;
    for(int j = 0; j<num_features; j++){
      regularization += weights[j] * weights[j];
    }
    regularization *= lambda_param;

    return regularization + loss / num_samples;
  }

  // Name: gradient_update
  // Purpose: Gradient descent process.
  // Return: void
  // Arguments: weights,
  //            bias,
  //            input data X
  //            target y
  //            number of samples (rows)
  //            number of features
  //            lambda parameter
  //            learning rate
  void gradient_update(double* weights, double* bias, const double* X, const double* y, int num_samples, int num_features, double lambda_param, double lr){
    double* weight_grad = calloc(num_features, sizeof(double));
    double bias_grad = 0.0f;

    for(int i = 0; i<num_samples; i++){
      double margin = y[i] * (dot_product(weights, &X[i * num_features], num_features) - *bias);

      if(margin < 1){
        // Update gradient
        for(int j = 0; j<num_features; j++){
          weight_grad[j] += -y[i] * X[i * num_features + j];
        }
        bias_grad += -y[i];
      }
    }

    // Regularization term for weights
    for(int j = 0; j<num_features; j++){
      weight_grad[j] += 2 * lambda_param * weights[j];
    }

    // Update weights and bias
    for(int j = 0; j<num_features; j++){
      weights[j] -= lr * weight_grad[j];
    }
    ,*bias -= lr * bias_grad;

    free(weight_grad);
  }

  // Name: predict_sample
  // Purpose: Predict class for a single sample.
  // Return: int
  // Arguments: weights,
  //            bias,
  //            sample,
  //            number of features
  int predict_sample(const double* weights, double bias, const double* sample, int num_features){
    double result = dot_product(weights, sample, num_features) - bias;
    return sign(result);
  }

  // Name: predict
  // Purpose: Make predictions.
  // Return: void
  // Arguments: weights,
  //            bias,
  //            input data X,
  //            predictions,
  //            number of samples (rows)
  //            number of features
  void predict(const double* weights, double bias, const double* X, int* predictions, int num_samples, int num_features){
    for(int i = 0; i<num_samples; i++){
      predictions[i] = predict_sample(weights, bias, &X[i * num_features], num_features);
    }
  }

  // Name: evaluate
  // Purpose: Evaluate accuracy of the model.
  // Return: double
  // Arguments: predictions,X
  //            target y,
  //            number of samples (rows)
  double evaluate(const int* predictions, const double* y, int num_samples){
    int correct = 0;

    for(int i = 0; i<num_samples; i++){
      if(predictions[i] == (int)y[i])
        correct++;
    }
    return (double)correct / num_samples * 100.0;
  }

  // Name: train
  // Purpose: Training loop.
  // Return: void
  // Arguments: weights,
  //            bias,
  //            input data X,
  //            target y,
  //            number of samples (rows),
  //            number of features,
  //            lambda parameter,
  //            learning rate,
  //            number of epochs
  void train(double* weights, double* bias, const double* X, const double* y, int num_samples, int num_features, double lambda_param, double lr, int num_epochs){
    for(int epoch = 0; epoch<num_epochs; epoch++){
      gradient_update(weights, bias, X, y, num_samples, num_features, lambda_param, lr);

      if(epoch % 100 == 0){
        double loss = hinge_loss(weights, *bias, X, y, num_samples, num_features, lambda_param);
        printf("Epoch %d, Loss: %.4f\n", epoch, loss);
      }
    }
  } 
#+end_src

#+RESULTS: svm_C

5) Main function and testing
#+begin_src C :main no :results output :noweb yes :tangle svm.c
  <<svm_C>>

  int main(){
    // Parameters
    const char* filename = "wdbc.data";
    const int max_samples = MAX_SAMPLES;
    const int num_features = MAX_FEATURES;
    const double test_size = 0.2;
    const double lambda_param = 0.001;
    const double lr = 0.00001;
    const int num_epochs = 1001;

    // Allocate memory
    Sample dataset[MAX_SAMPLES];
    double* X = malloc(max_samples * num_features * sizeof(double));
    double* y = malloc(max_samples * sizeof(double));
    double* X_train = malloc(max_samples * num_features * sizeof(double));
    double* X_test = malloc(max_samples * num_features * sizeof(double));
    double* y_train = malloc(max_samples * sizeof(double));
    double* y_test = malloc(max_samples * sizeof(double));

    // Load the dataset
    int total_samples = load_data(filename, dataset, max_samples);
    if(total_samples <= 0){
      printf("Failed to load dataset.\n");
      return -1;
    }
    printf("Loaded %d samples.\n", total_samples);

    // Prepare the data
    for(int i = 0; i<total_samples; i++){
      for(int j = 0; j<num_features; j++){
        X[i * num_features + j] = dataset[i].features[j];
      }
      y[i] = dataset[i].label;
    }

    // Split the data
    int num_train_samples = (int)(total_samples * (1 - test_size));
    int num_test_samples = total_samples - num_train_samples;
    train_test_split(X, y, X_train, X_test, y_train, y_test, total_samples, num_features, test_size, 1618);

    // Standardize the data
    StandardScaler scaler; 
    scaler_fit_transform(&scaler, X_train, num_train_samples, num_features);
    scaler_transform(&scaler, X_test, num_test_samples);

    // Initialize weights and bias
    double* weights = calloc(num_features, sizeof(double));
    double bias = 0.0;

    // Train the model
    train(weights, &bias, X_train, y_train,num_train_samples, num_features, lambda_param, lr, num_epochs);

    // Make predictions
    int* predictions = malloc(num_test_samples * sizeof(int));
    predict(weights, bias, X_test, predictions, num_test_samples, num_features);

    // Evaluate the model
    double accuracy = evaluate(predictions, y_test, num_test_samples);
    printf("Model Accuracy: %.2f%%\n", accuracy);

    // Free allocated memory
    free(weights);
    free(predictions);
    free(X);
    free(y);
    free(X_train);
    free(X_test);
    free(y_train);
    free(y_test);
    scalar_free(&scaler);

    return 0;
    }
#+end_src

#+RESULTS:

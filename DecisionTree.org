#+TITLE: Decision Tree Regressor from Scratch
#+AUTHOR: Cristian Del Gobbo (pledged)
#+STARTUP: overview hideblocks indent
#+property: header-args:python :python python3 :session *Python* :results output :exports both :noweb yes :tangle yes:

* Introduction
In this notebook, I will implement a Decision Tree Regressor algorithm 
from scratch using Python and C, without relying on any ML related external libraries.
* Algorithm Description
* Code 
** Python Code
Import the data (DESCRIPTION OF THE DATASET)
#+name: data
#+begin_src python :python python3 :results output
import numpy as np
import pandas as pd
import os

dir = os.getcwdb().decode('utf-8')
file_path = os.path.join(dir, "Datasets/parkinsons/parkinsons_updrs.data")

# Coulmns names
column_names = [
    "subject#", "age", "sex", "test_time", "motor_UPDRS", "total_UPDRS", 
    "Jitter(%)", "Jitter(Abs)", "Jitter:RAP", "Jitter:PPQ5", "Jitter:DDP", 
    "Shimmer", "Shimmer(dB)", "Shimmer:APQ3", "Shimmer:APQ5", 
    "Shimmer:APQ11", "Shimmer:DDA", "NHR", "HNR", "RPDE", "DFA", 
    "PPE"
]

# Import the data
data = pd.read_csv(file_path, sep=",", header=None, names=column_names)
# Remove first row
data = data[1:-1]

X = data.drop("total_UPDRS", axis=1)
X = X.apply(pd.to_numeric, errors="coerce")
y = data["total_UPDRS"]
y = y.apply(pd.to_numeric, errors="coerce")
#print(X.dtypes)
#print(y.dtypes)
#+end_src

#+RESULTS: data
#+begin_example
subject#           int64
age                int64
sex                int64
test_time        float64
motor_UPDRS      float64
Jitter(%)        float64
Jitter(Abs)      float64
Jitter:RAP       float64
Jitter:PPQ5      float64
Jitter:DDP       float64
Shimmer          float64
Shimmer(dB)      float64
Shimmer:APQ3     float64
Shimmer:APQ5     float64
Shimmer:APQ11    float64
Shimmer:DDA      float64
NHR              float64
HNR              float64
RPDE             float64
DFA              float64
PPE              float64
dtype: object
float64
#+end_example

Split the data and standardize them.
#+name: stand
#+begin_src python :python python3 :results output
<<data>>
# Create function to split the data (similar to scikit-learn train_test_split)
def train_test_split(X, y, test_size=0.2, random_state=None):
    if random_state:
        np.random.seed(random_state)

    # Shuffle data
    indices = np.arange(X.shape[0])
    np.random.shuffle(indices)

    X_shuffled = X.iloc[indices]
    y_shuffled = y.iloc[indices]

    split_index = int(X.shape[0] * (1 - test_size))

    X_train, X_test = X_shuffled[:split_index], X_shuffled[split_index:]
    y_train, y_test = y_shuffled[:split_index], y_shuffled[split_index:]

    return X_train, X_test, y_train, y_test

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, 0.2, random_state=1618)

# Create a custom Standard Scaler Class (To replicate the scikit-learn class "StandardScaler")
class StandardScaler:
    def __init__(self):
        self.mean = None
        self.std = None

    def fit(self, X):
        self.mean = np.mean(X, axis=0)
        self.std = np.std(X, axis=0)

    def transform(self, X):
        return (X - self.mean) / self.std

    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)
 
# Apply to the dataset
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
#+end_src

#+RESULTS: stand

Create the Decision Tree regressor model.
#+name: model
#+begin_src python :python python3 :results output
<<stand>>
# Decison Tree class
class DecisionTreeRegressor:
    def __init__(self, max_depth=10, min_samples=2):
        self.max_depth = max_depth
        self.min_samples = min_samples
        self.tree = None

    def train(self, X, y):
        self.tree = self._build_tree(X, y)

    def predict(self, X):
        return np.array([self._traverse_tree(x, self.tree) for x in X])
    
    def _build_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        if depth >= self.max_depth or n_samples < self.min_samples_split:
            return {"value": np.mean(y)}
     
        best_feature, best_threshold, best_mse = self._find_best_split(X, y)
        
        if best_feature is None:
            return {"value": np.mean(y)}
  
        left_indices = X[:, best_feature] <= best_threshold
        right_indices = X[:, best_feature] > best_threshold
  
        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)
        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)

        return {
            "feature": best_feature,
            "threshold": best_threshold,
            "left": left_subtree,
            "right": right_subtree,
        }

    def _find_best_split(self, X, y):
        n_samples, n_features = X.shape
        if n_samples <= 1:
            return None, None, float("inf")

        best_mse = float("inf")
        best_feature, best_threshold = None, None

        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                mse = self._calculate_split_mse(X, y, feature, threshold)
                if mse < best_mse:
                    best_mse = mse
                    best_feature = feature
                    best_threshold = threshold
        
        return best_feature, best_threshold, best_mse

    def _calculate_split_mse(self, X, y, feature, threshold):
        left_indices = X[:, feature] <= threshold
        right_indices = X[:, feature] > threshold

        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:
            return float("inf")

        left_mse = self.calculate_mse(y[left_indices])
        right_mse = self.calculate_mse(y[right_indices])

        return (left_mse * len(y[left_indices]) + right_mse * len(y[right_indices])) / len(y)

    def _calculate_mse(self, y):
        if len(y) == 0:
            return 0
        return np.mean((y - np.mean(y)) ** 2)

    def _traverse_tree(self, sample, node):
        if "value" in node:
            return node["value"]

        if sample[node["feature"]] <= node["threshold"]:
            return self._traverse_tree(sample, node["left"])
        else:
            return self._traverse_tree(sample, node["right"]) 
#+end_src

#+RESULTS: model

** C Code

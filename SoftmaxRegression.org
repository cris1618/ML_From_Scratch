#+TITLE: Softmax Regression from Scratch 
#+AUTHOR: Cristian Del Gobbo (pledged)
#+STARTUP: overview hideblocks indent
#+property: header-args:python :python python3 :session *Python* :results output :exports both :noweb yes :tangle yes:

* Introduction
In this notebook, I will implement a Softmax Regression algorithm 
from scratch using Python and C, without relying on any external libraries.
* Code
** Python Code
Step 1: Creating the data.
#+name: Data
#+begin_src python :python python3 :results output
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Set random seed
np.random.seed(1618)

# Dataset properties
num_rows = 10000
num_features = 4
num_classes = 3

# Create data points from distinct Gaussian distributions for each class
means = [
   [2, 2, 2, 2],
   [6, 6, 6, 6],
   [10, 10, 10, 10]
]
cov = np.eye(num_features) # Identity matrix as covariance (indipendent features)

X = []
y = []

# Generate 100 examples distributed across the 3 classes
examples_per_class = num_rows // num_classes

for class_label, mean in enumerate(means):
    class_data = np.random.multivariate_normal(mean, cov, examples_per_class)
    X.append(class_data)
    y.extend([class_label] * examples_per_class)

# Combine all features into a single array
X = np.vstack(X)
y = np.array(y)

# One-Hot Encode the Labels
def one_hot_encode(labels, num_classes):
    one_hot = np.zeros((labels.size, num_classes))
    one_hot[np.arange(labels.size), labels] = 1
    return one_hot

y_one_hot = one_hot_encode(y, num_classes)

# Include feature columns and the target class label
columns = [f"Feture_{i+1}" for i in range(num_features)] + ["Class"]
data = np.hstack((X, y.reshape(-1,1)))
df = pd.DataFrame(data, columns=columns)
#print(df.head())

# Create function to split the data (similar to scikit-learn train_test_split)
def train_test_split(X, y, test_size=0.2, random_state=None):
    if random_state:
        np.random.seed(random_state)
    
    # Shuffle data
    indices = np.arange(X.shape[0])
    np.random.shuffle(indices)
    
    X_shuffled = X[indices]
    y_shuffled = y[indices]
   
    split_index = int(X.shape[0] * (1 - test_size))

    X_train, X_test = X_shuffled[:split_index], X_shuffled[split_index:]
    y_train, y_test = y_shuffled[:split_index], y_shuffled[split_index:]
    
    return X_train, X_test, y_train, y_test

# Split the Data
X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=1618)
#+end_src

#+RESULTS: Data
: Feture_1  Feture_2  Feture_3  Feture_4  Class
: 0  0.419980  2.835402  2.635347  1.179549    0.0
: 1  1.601603  1.964513  3.864144  1.512772    0.0
: 2  2.423905  1.488458  2.111663  1.122973    0.0
: 3  1.866911  0.848720  0.983183  3.816334    0.0
: 4  1.966817  2.513222  3.587602  0.704391    0.0

Visualizing the data to have an idea of the dataset' composition
#+name: viz_data
#+begin_src python :file softmax.png :python python3 :session *Python* :results output graphics file
<<Data>>
fig, ax = plt.subplots(1,1,figsize=(8,6))
for class_label in range(num_classes):
    ax.scatter(
        X[y == class_label, 0],  # Feature 1
        X[y == class_label, 1],  # Feature 2
        label=f"Class {class_label}" 
    )

ax.set_title("Scatter Plot of First Two Features")
ax.set_xlabel("Feature 1")
ax.set_ylabel("Feature 2")
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)


plt.legend()
plt.grid()
plt.savefig("softmax.png")
plt.show()
#+end_src

#+RESULTS: viz_data
[[file:softmax.png]]

Step 2: Implement Softmax Function, Cross-entropy Loss and Gradient Descent.
#+name: functions
#+begin_src python :python python3 :results output
<<Data>>
# Softmax function
def softmax(theta, bias, X):
    z = np.dot(X, theta.T) + bias #Shape: (num_samples, num_classes)

    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True)) # Subtract max for numerical stability
    softmax_probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)

    return softmax_probs

# Define the loss function
def loss_function(theta, bias, X, Y):
    softmax_probs = softmax(theta, bias, X)
    
    #Compute the cross entropy loss
    log_probs = np.log(softmax_probs)
    loss = -np.sum(Y * log_probs) / X.shape[0]
    
    return loss

# Gradient Descent function 
def gradient_descent(theta, bias, X, Y, lr):
    softmax_probs = softmax(theta, bias, X)
    theta_grad = np.dot((softmax_probs - Y).T, X) / X.shape[0]
    bias_grad = np.sum(softmax_probs - Y, axis=0) / X.shape[0]
    
    # Update the parameters
    theta = theta - lr * theta_grad
    bias = bias - lr * bias_grad
    
    return theta, bias
#+end_src

#+RESULTS: functions

Step 3: Training and Results 
#+name: train
#+begin_src python :python python3 :results output 
<<functions>>
theta = np.random.rand(num_classes, num_features)
bias = np.random.rand(num_classes)
lr = 0.01
num_epochs = 3000

for i in range(num_epochs):
    #if i % 100 == 0:
        #print(f"Epoch: {i}, Loss: {loss_function(theta, bias, X, y_one_hot)}")
    theta, bias = gradient_descent(theta, bias, X_train, y_train, lr)

#print(theta, bias)
#+end_src

#+RESULTS: train
#+begin_example
Epoch: 0, Loss: 1.9700309176164128
Epoch: 100, Loss: 0.9944775992936902
Epoch: 200, Loss: 0.9096532898218269
Epoch: 300, Loss: 0.8464811355125262
Epoch: 400, Loss: 0.7960397045181783
Epoch: 500, Loss: 0.753915268988989
Epoch: 600, Loss: 0.7177726257488833
Epoch: 700, Loss: 0.6862268186327652
Epoch: 800, Loss: 0.6583618096228339
Epoch: 900, Loss: 0.6335201295089731
Epoch: 1000, Loss: 0.6112036348595189
Epoch: 1100, Loss: 0.5910216546290057
Epoch: 1200, Loss: 0.5726608708575179
Epoch: 1300, Loss: 0.5558661220424945
Epoch: 1400, Loss: 0.5404272555341268
Epoch: 1500, Loss: 0.5261696457548282
Epoch: 1600, Loss: 0.5129471067999758
Epoch: 1700, Loss: 0.5006364615491323
Epoch: 1800, Loss: 0.4891333060888794
Epoch: 1900, Loss: 0.47834866313573776
Epoch: 2000, Loss: 0.4682063112109795
Epoch: 2100, Loss: 0.4586406357647434
Epoch: 2200, Loss: 0.44959488837418915
Epoch: 2300, Loss: 0.44101976804173665
Epoch: 2400, Loss: 0.43287225872085844
Epoch: 2500, Loss: 0.42511467201928715
Epoch: 2600, Loss: 0.41771385515490944
Epoch: 2700, Loss: 0.4106405327052543
Epoch: 2800, Loss: 0.40386875720319576
Epoch: 2900, Loss: 0.3973754486840576
Epoch: 3000, Loss: 0.39114000723825104
Epoch: 3100, Loss: 0.38514398572911573
Epoch: 3200, Loss: 0.3793708122912311
Epoch: 3300, Loss: 0.3738055541756722
Epoch: 3400, Loss: 0.3684347160661342
Epoch: 3500, Loss: 0.3632460672381991
Epoch: 3600, Loss: 0.3582284929386506
Epoch: 3700, Loss: 0.35337186617330896
Epoch: 3800, Loss: 0.348666936749879
Epoch: 3900, Loss: 0.3441052349577478
Epoch: 4000, Loss: 0.33967898770386545
Epoch: 4100, Loss: 0.3353810452820679
Epoch: 4200, Loss: 0.3312048172476353
Epoch: 4300, Loss: 0.3271442161117617
Epoch: 4400, Loss: 0.3231936077715017
Epoch: 4500, Loss: 0.31934776775753454
Epoch: 4600, Loss: 0.31560184252089757
Epoch: 4700, Loss: 0.311951315095763
Epoch: 4800, Loss: 0.30839197457241174
Epoch: 4900, Loss: 0.30491988889609045
[[0.17181794 0.4329654  0.33394459 0.21769787]
 [0.29702252 0.39270683 0.82204791 0.62884188]
 [0.49084321 0.84485356 0.82669333 0.47993371]] [ 4.08642427  0.629978   -2.97738098]
#+end_example

Step 4: Evaluate the model
#+name: evaluate
#+begin_src  python :python python3 :results output 
<<train>>
# Function to make prdictions
def predict(theta, bias, X):
    # Compute softmax probabilities
    softmax_probs = softmax(theta, bias, X)
    
    # Select the class with the highest probability
    predictions = np.argmax(softmax_probs, axis=1)
    return predictions

# Accuracy computation
def evaluate_accuracy(theta, bias, X, y_true):
    predictions = predict(theta, bias, X)

    # Convert one-hot to class indicies
    y_true_classes = np.argmax(y_true, axis=1)

    accuracy = np.sum(predictions == y_true_classes) / y_true_classes.shape[0]
    return accuracy

# Evaluate accuracy of the model 
accuracy_test = evaluate_accuracy(theta, bias, X_test, y_test)
print(f"Model Accuracy (test): {accuracy_test * 100:.2f}%")

accuracy_train = evaluate_accuracy(theta, bias, X_train, y_train)
print(f"Model Accuracy (train): {accuracy_train * 100:.2f}%")
#+end_src

#+RESULTS: evaluate
: Model Accuracy (test): 98.15%
: Model Accuracy (train): 98.46%

** C Code
Now let's recreate the same softmax algorithm in C,
following the same steps as the Python implementation.

Functions definition to generate and split the data.
#+name: funCs_data
#+begin_src C :results none :noweb yes :includes <stdio.h> <math.h> 
  // Include libraries 
  #include <stdlib.h>
  #include <time.h>

  // Name: eye
  // Purpose: Create an identity matrix.
  // Return: void
  // Arguments: Number of features and 2D array.
  // Notes: Replication of NumPy "np.eye" function.
  void eye(int num_features, int arr[][num_features]){
    for(int i = 0; i<num_features; i++){
      for(int j = 0; j<num_features; j++){
        arr[i][j] = (i == j) ? 1 : 0;
      }
    }
  }

  // Name: generate_data
  // Purpose: Create gaussian distributed data
  // Return: void
  // Arguments: Number of rows, Number of features
  // Number of classes, Features array X and target y
  void generate_data(int num_rows, int num_features, int num_classes, double X[][num_features], int y[]){
    int examples_per_class = num_rows / num_classes;

    // Further generalizible, example purpose
    // Mean for each class
    double means[3][4] = {
      {2.0, 2.0, 2.0, 2.0},
      {10.0, 10.0, 10.0, 10.0},
      {25.0, 25.0, 25.0, 25.0}
    };

    // seed random generator
    srand(time(0));

    // Generate the data
    for(int class_label = 0; class_label < num_classes; class_label++){
      for(int i = 0; i<examples_per_class; i++){
        int index = class_label * examples_per_class + i;
        for(int j = 0; j<num_features; j++){
          // Generate random data around the mean (Gaussian)
           double noise = ((double)rand() / RAND_MAX) * 1.5 - 0.75; // add noise
           X[index][j] = means[class_label][j] + noise;
          //X[index][j] = ((rand() % 10) + class_label * 4) / 10.0;
        }
        y[index] = class_label;
      }
    }
  }

  // Name: one_hot_encode
  // Purpose: Create one hot encoded target array
  // Return: void
  // Arguments: Target array y, Number of rows, 
  // Number of classes, Target one hot encoded empty array
  void one_hot_encode(int y[], int num_rows, int num_classes, double y_one_hot[][num_classes]){
    for(int i = 0; i<num_rows; i++){
      for(int j = 0; j<num_classes; j++){
        y_one_hot[i][j] = (y[i] == j) ? 1 : 0;
      }
    }
  }

  // Name: shuffle
  // Purpose: Shuffle indices.
  // Return: void
  // Arguments: indices,
  //            Number of rows (total number of indices to shuffle),
  //            Random State.
  void shuffle(int* indices, int num_rows, int random_state){
    srand(random_state);
    for(int i = num_rows - 1; i > 0; i--){
      int j = rand() % (i+1);
      int temp = indices[i];
      indices[i] = indices[j];
      indices[j] = temp;
    }
  }

  // Name: train_test_split
  // Purpose: Split the data for training and for testing.
  // Return: void
  // Arguments: X to split,
  //            y to split,
  //            X_train, X_test, y_train, y_test (Outputs),
  //            Number of rows,
  //            Number of features,
  //            Test size,
  //            Random State.
  void train_test_split(double* X, double* y, double* X_train, double* X_test, double* y_train, double* y_test, int num_rows, int num_features, int num_classes, double test_size, int random_state){
    int indices[num_rows];
    for(int i = 0; i<num_rows; i++){
      indices[i] = i;
    }

    shuffle(indices, num_rows, random_state);

    int split_index = (int)(num_rows * (1 - test_size));

    // Perform split
    for(int i = 0; i<split_index; i++){
      int idx = indices[i];
      for(int j = 0; j<num_features; j++){
        X_train[i * num_features + j] = X[idx * num_features + j];
      }
      for(int j = 0; j<num_classes; j++)
        y_train[i * num_classes + j] = y[idx * num_classes + j];
    }

    for(int i = split_index; i<num_rows; i++){
      int idx = indices[i];
      for(int j = 0; j<num_features; j++){
        X_test[(i - split_index) * num_features + j] = X[idx * num_features + j];
      }
      for(int j = 0; j<num_classes; j++)
        y_test[(i - split_index) * num_classes + j] = y[idx * num_classes + j];
    }
  }

#+end_src

Define helper functions for operation with matrices and functions (softmax in this case) computation.
#+name: funC_help
#+begin_src C :results none :noweb yes :includes <stdio.h> <math.h>
  // Name: dot_product
  // Purpose: Compute the dot product of two matrices.
  // Return: void
  // Arguments: Flatten 1D result matrix,
  //            Flatten 1D matrix 1,
  //            Flatten 1D matrix 2,
  //            Number of rows first matrix,
  //            Number of columns first matrix,
  //            Number of columns second matrix.
  // Notes: Replication of NumPy "np.dot" function.  
  void dot_product(double* result, double* matrix1, double* matrix2, int rows1, int cols1, int cols2){
    for(int i = 0; i<rows1; i++){
      for(int j = 0; j<cols2; j++){
        result[i * cols2 + j] = 0.0;
        for(int k = 0; k<cols1; k++){
          result[i * cols2 + j] += matrix1[i * cols1 + k] * matrix2[k * cols2 + j];
        }
      }
    }
  }

  // Name: softmax
  // Purpose:  Apply softmax function.
  // Return: void
  // Arguments: Flatten 1D output matrix,
  //            Flatten 1D logits matrix,
  //            Number of rows logits matrix,
  //            Number of columns logits matrix.
  void softmax(double* output, double* logits, int rows, int cols){
    for(int i = 0; i<rows; i++){
      double max_val = logits[i * cols];
      for(int j = 1; j<cols; j++){
        if(logits[i * cols + j] > max_val){
          max_val = logits[i * cols + j];
        }
      }

      double sum_exp = 0.0;
      for(int j = 0; j<cols; j++){
        output[i * cols + j] = exp(logits[i * cols + j] - max_val); // For numerical stability
        sum_exp += output[i * cols + j];
      }

      for(int j = 0; j<cols; j++){
        output[i * cols + j] /=  sum_exp;
      }
    }
  }

  // Name: log_softmax
  // Purpose: Compute element_wise logarithm of softmax probabilities.
  // Return: void
  // Arguments: Flatten 1D output matrix,
  //            Flatten 1D logits matrix,
  //            Number of rows logits matrix,
  //            Number of columns logits matrix.
  void log_softmax(double* output, double* softmax_probs, int rows, int cols){
    for(int i = 0; i<rows*cols; i++){
      output[i] = log(softmax_probs[i]);
    }
  }

  // Name: matrix_subtract
  // Purpose: Subtract two matrices
  // Return: void
  // Arguments: Flatten 1D result matrix,
  //            Flatten 1D matrix 1,
  //            Flatten 1D matrix 2,
  //            Number of rows matrix 1 and 2 (same dimensions),
  //            Number of columns matrix 1 and 2.
  void matrix_subtract(double* result, double* matrix1, double* matrix2, int rows, int cols){
    for(int i = 0; i<rows*cols; i++){
      result[i] = matrix1[i] - matrix2[i];
    }
  }

  // Name: transpose
  // Purpose: Transpose a matrix
  // Return: void
  // Arguments: Flatten 1D result matrix,
  //            Flatten 1D matrix to transpose,
  //            Number of rows of the matrix,
  //            Number of columns of the matrix.
  void transpose(double* result, double* matrix, int rows, int cols){
    for(int i = 0; i<rows; i++){
      for(int j = 0; j<cols; j++){
        result[j * rows + i] = matrix[i * cols + j];
      }
    }
  }

  // Name: argmax
  // Purpose: Find index which corresponds to the maxiumum value of an array
  // Return: int
  // Arguments: Array,
  //            Array's size
  int argmax(double* arr, int size){
    int max_index = 0;
    double max_value = arr[0];

    for(int i = 0; i<size; i++){
      if(arr[i] > max_value){
        max_value = arr[i];
        max_index = i;
      }
    }
    return max_index;
  } 
#+end_src

Functions definition for Softmax, Cross-entropy loss, Gradient descent, compute predictions and accuracy.
#+name: funC_soft
#+begin_src C :results none :noweb yes
  <<funC_help>>

    // Name: compute_softmax
    // Purpose: Compute the softmax function.
    // Return: void
    // Arguments: Flatten 1D softmax probabilities matrix,
    //            Flatten 1D parameter theta matrix,
    //            Flatten 1D bias matrix,
    //            Flatten 1D inputs matrix,
    //            Number of rows (samples),
    //            Number of features,
    //            Number of classes.
  void compute_softmax(double* softmax_probs, double* theta, double* bias, double* X, int num_rows, int num_features, int num_classes){
    double logits[num_rows * num_classes];

    // Compute logits: z = X * theta.T + bias 
    dot_product(logits, X, theta, num_rows, num_features, num_classes);

    // Add the bias to logits
    for (int i = 0; i < num_rows; i++) {
      for (int j = 0; j < num_classes; j++) {
        logits[i * num_classes + j] += bias[j];
      }
    }

    // Stabilize logits using the log-sum-exp trick
    for (int i = 0; i < num_rows; i++) {
      // Step 1: Find max logit for each row (sample)
      double max_logit = logits[i * num_classes];
      for (int j = 1; j < num_classes; j++) {
        if (logits[i * num_classes + j] > max_logit) {
          max_logit = logits[i * num_classes + j];
        }
      }

      // Step 2: Subtract max logit and compute exp
      double sum_exp = 0.0;
      for (int j = 0; j < num_classes; j++) {
        softmax_probs[i * num_classes + j] = exp(logits[i * num_classes + j] - max_logit);  // Shift for stability
        sum_exp += softmax_probs[i * num_classes + j];
      }

      // Step 3: Normalize the probabilities
      for (int j = 0; j < num_classes; j++) {
        softmax_probs[i * num_classes + j] /= sum_exp;
      }
    }
    }

  // Name: compute_loss
  // Purpose: Calculate the cross-entropy loss.
  // Return: double loss
  // Arguments: Flatten 1D parameter theta matrix,
  //            Flatten 1D bias matrix,
  //            Flatten 1D inputs matrix,
  //            1D targets vector,
  //            Number of rows (samples),
  //            Number of features,
  //            Number of classes.
  double compute_loss(double* theta, double* bias, double* X, double* Y, int num_rows, int num_features, int num_classes){
    double softmax_probs[num_rows * num_classes];
    compute_softmax(softmax_probs, theta, bias, X, num_rows, num_features, num_classes);

    double log_probs[num_rows * num_classes];
    log_softmax(log_probs, softmax_probs, num_rows, num_classes);

    double loss = 0.0;
    for(int i = 0; i<num_rows*num_classes; i++){
      if(Y[i] > 0)
        loss += Y[i] * log_probs[i];
    } 
    return -loss / num_rows;
  }

  // Name: compute_loss
  // Purpose: Compute the Gradient Descent.
  // Return: void
  // Arguments: Flatten 1D parameter theta matrix,
  //            Flatten 1D bias matrix,
  //            Flatten 1D inputs matrix,
  //            1D targets vector,
  //            Number of rows (samples),
  //            Number of features,
  //            Number of classes.
  //            Learning rate.
  void gradient_descent(double* theta, double* bias, double* X, double* Y, int num_rows, int num_features, int num_classes, double lr){
    double softmax_probs[num_rows * num_classes];
    compute_softmax(softmax_probs, theta, bias, X, num_rows, num_features, num_classes);

    // Compute gradients
    double grad_probs[num_rows * num_classes];
    matrix_subtract(grad_probs, softmax_probs, Y, num_rows, num_classes);

    double theta_grad[num_classes * num_features];
    double grad_probs_T[num_classes * num_rows]; // num_features
    transpose(grad_probs_T, grad_probs, num_rows, num_classes); // num_features

    dot_product(theta_grad, grad_probs_T, X, num_classes, num_rows, num_features);
   
    double bias_grad[num_classes];
    for(int i = 0; i<num_classes; i++){
      bias_grad[i] = 0.0;
      for(int j = 0; j<num_rows; j++){
        bias_grad[i] += (softmax_probs[j * num_classes + i] - Y[j * num_classes + i]);
      }
    }

    double lambda = 0.01; // L2 Regularization
    for(int i = 0; i<num_classes * num_features; i++){
      theta[i] -= lr * (theta_grad[i] / num_rows + lambda * theta[i]);
    }
    for(int i = 0; i<num_classes; i++){
      bias[i] -= lr * bias_grad[i] / num_rows;
    }
  }

  // Name: predict
  // Purpose: Infer predictions on new data
  // Return: void
  // Arguments: Predictions array
  //            Flatten 1D parameter theta matrix,
  //            Flatten 1D bias matrix,
  //            Flatten 1D inputs matrix,
  //            Number of rows (samples),
  //            Number of features,
  //            Number of classes.
  void predict(int* predictions, double* theta, double* bias, double* X, int num_rows, int num_features, int num_classes){
    double softmax_probs[num_rows * num_classes];
    compute_softmax(softmax_probs, theta, bias, X, num_rows, num_features, num_classes);

    for(int i = 0; i<num_rows; i++){
      predictions[i] = argmax(&softmax_probs[i * num_classes], num_classes);
    }

  }

  // Name: evaluate_accuracy
  // Purpose: Evaluate the accuracy of the model.
  // Return: double
  // Arguments: Flatten 1D parameter theta matrix,      
  //            Flatten 1D bias matrix,
  //            Flatten 1D inputs matrix,
  //            Flatten 1D target data,
  //            Number of rows (samples),
  //            Number of features,
  //            Number of classes.
  double evaluate_accuracy(double* theta, double* bias, double* X, double* y_true, int num_rows, int num_features, int num_classes){
    int predictions[num_rows];
    int correct = 0;

    // Get predictions
    predict(predictions, theta, bias, X, num_rows, num_features, num_classes);

    // Compare predictions with targets
    for(int i = 0; i<num_rows; i++){
      int true_class = argmax(&y_true[i * num_classes], num_classes);
      if(predictions[i] == true_class){
        correct++;
      }
    }
    return (double)correct/num_rows;
  }
#+end_src

Training and Accuracy computation.
#+name: Main
#+begin_src C :exports none :main no :noweb yes :cmdline -lm :includes <stdio.h> <math.h> :tangle soft.c
  // Include function definitions
  <<funCs_data>>
  <<funC_soft>>

  int main(){
    // Dataset properties
    int num_rows = 1000;
    int num_features = 4;
    int num_classes = 3;
    double split_size = 0.2;
    int random_state = 1618;
    int train_size = (int)(num_rows * (1 - split_size));
    int test_size = (int)(num_rows * split_size);

    // Training parameters
    int num_epochs = 10000;
    double lr = 0.01; // 0.001

    // Define Arrays
    double X_2D[num_rows][num_features];
    int y[num_rows];
    double y_one_hot_2D[num_rows][num_classes];

    // Flattened Arrays
    double X[num_rows * num_features];
    double y_one_hot[num_rows * num_classes];

    // Train and test Arrays
    double X_train[(int)(num_rows * (1 - split_size)) * num_features];
    double X_test[(int)(num_rows * split_size) * num_features];
    double y_train[(int)(num_rows * (1 - split_size)) * num_classes];
    double y_test[(int)(num_rows * split_size) * num_classes];

    // Parameters to optimize
    double theta[num_features * num_classes];
    double bias[num_classes];

    // Initialize parameters randomly
    srand(time(0));
    for(int i = 0; i<num_features*num_classes; i++){
      theta[i] = ((double)rand() / RAND_MAX) * 0.01;
    }

    for(int i = 0; i<num_classes; i++){
      bias[i] = 0.0;
    }

    // Generate data
    generate_data(num_rows, num_features, num_classes, X_2D, y);

    // One-hot-encode the labels
    one_hot_encode(y, num_rows, num_classes, y_one_hot_2D);

    // Flatten 2D arrays
    for(int i = 0; i<num_rows; i++){
      for(int j = 0; j<num_features; j++){
        X[i * num_features + j] = X_2D[i][j];
      }
      for(int j = 0; j<num_classes; j++){
        y_one_hot[i * num_classes + j] = y_one_hot_2D[i][j];
      }
    }
   
    // Split the data
    train_test_split(X, y_one_hot, X_train, X_test, y_train, y_test, num_rows, num_features, num_classes, split_size, random_state);

    // Training loop
    for(int epoch = 0; epoch<num_epochs; epoch++){
      if(epoch % 100 == 0){
        double loss = compute_loss(theta, bias, X_train, y_train, train_size, num_features, num_classes);
        printf("Epoch: %d,\tLoss: %.4f\n", epoch, loss);
      }
      gradient_descent(theta, bias, X_train, y_train, train_size, num_features, num_classes, lr);
    }

    printf("Final theta values:\n");
    for (int i = 0; i < num_features * num_classes; i++) {
      printf("%.4f ", theta[i]);
      if ((i + 1) % num_features == 0) printf("\n");
    }

    printf("Final bias values:\n");
    for (int i = 0; i < num_classes; i++) {
      printf("%.4f ", bias[i]);
    }
    printf("\n");

    // Evaluate model accuracy
    // Train set
    double train_acc = evaluate_accuracy(theta, bias, X_train, y_train, train_size, num_features, num_classes);
    printf("Model accuracy on training set: %.2f%%\n", train_acc*100);

    double test_acc = evaluate_accuracy(theta, bias, X_test, y_test, test_size, num_features, num_classes);
    printf("Model accuracy on testing set: %.2f%%\n", test_acc*100);


    return 0;
    }
#+end_src

#+RESULTS: Main



#+TITLE: Softmax Regression from Scratch 
#+AUTHOR: Cristian Del Gobbo (pledged)
#+STARTUP: overview hideblocks indent
#+property: header-args:python :python python3 :session *Python* :results output :exports both :noweb yes :tangle yes:

* Introduction
In this notebook, I will implement a Softmax Regression algorithm 
from scratch using Python and C, without relying on any external libraries.
* Code
** Python Code
Step 1: Creating the data.
#+name: Data
#+begin_src python :python python3 :results output
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Set random seed
np.random.seed(1618)

# Dataset properties
num_rows = 100
num_features = 4
num_classes = 3

# Create data points from distinct Gaussian distributions for each class
means = [
   [2, 2, 2, 2],
   [6, 6, 6, 6],
   [10, 10, 10, 10]
]
cov = np.eye(num_features) # Identity matrix as covariance (indipendent features)

X = []
y = []

# Generate 100 examples distributed across the 3 classes
examples_per_class = num_rows // num_classes

for class_label, mean in enumerate(means):
    class_data = np.random.multivariate_normal(mean, cov, examples_per_class)
    X.append(class_data)
    y.extend([class_label] * examples_per_class)

# Combine all features into a single array
X = np.vstack(X)
y = np.array(y)

# One-Hot Encode the Labels
def one_hot_encode(labels, num_classes):
    one_hot = np.zeros((labels.size, num_classes))
    one_hot[np.arange(labels.size), labels] = 1
    return one_hot

y_one_hot = one_hot_encode(y, num_classes)

# Include feature columns and the target class label
columns = [f"Feture_{i+1}" for i in range(num_features)] + ["Class"]
data = np.hstack((X, y.reshape(-1,1)))
df = pd.DataFrame(data, columns=columns)
#print(df.head())
#+end_src

#+RESULTS: Data
: Feture_1  Feture_2  Feture_3  Feture_4  Class
: 0  0.419980  2.835402  2.635347  1.179549    0.0
: 1  1.601603  1.964513  3.864144  1.512772    0.0
: 2  2.423905  1.488458  2.111663  1.122973    0.0
: 3  1.866911  0.848720  0.983183  3.816334    0.0
: 4  1.966817  2.513222  3.587602  0.704391    0.0

Visualizing the data to have an idea of the dataset' composition
#+name: viz_data
#+begin_src python :file softmax.png :python python3 :session *Python* :results output graphics file
<<Data>>
fig, ax = plt.subplots(1,1,figsize=(8,6))
for class_label in range(num_classes):
    ax.scatter(
        X[y == class_label, 0],  # Feature 1
        X[y == class_label, 1],  # Feature 2
        label=f"Class {class_label}" 
    )

ax.set_title("Scatter Plot of First Two Features")
ax.set_xlabel("Feature 1")
ax.set_ylabel("Feature 2")
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)


plt.legend()
plt.grid()
plt.savefig("softmax.png")
plt.show()
#+end_src

#+RESULTS: viz_data
[[file:softmax.png]]

Step 2: Implement Softmax Function, Cross-entropy Loss and Gradient Descent.
#+name: functions
#+begin_src python :python python3 :results output
<<Data>>
# Softmax function
def softmax(theta, bias, X):
    z = np.dot(X, theta.T) + bias #Shape: (num_samples, num_classes)

    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True)) # Subtract max for numerical stability
    softmax_probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)

    return softmax_probs

# Define the loss function
def loss_function(theta, bias, X, Y):
    softmax_probs = softmax(theta, bias, X)
    
    #Compute the cross entropy loss
    log_probs = np.log(softmax_probs)
    loss = -np.sum(Y * log_probs) / X.shape[0]
    
    return loss

# Gradient Descent function 
def gradient_descent(theta, bias, X, Y, lr):
    softmax_probs = softmax(theta, bias, X)
    theta_grad = np.dot((softmax_probs - Y).T, X) / X.shape[0]
    bias_grad = np.sum(softmax_probs - Y, axis=0) / X.shape[0]
    
    # Update the parameters
    theta = theta - lr * theta_grad
    bias = bias - lr * bias_grad
    
    return theta, bias
#+end_src

#+RESULTS: functions

Step 3: Training 
#+begin_src python :python python3 :results output 
<<functions>>
theta = np.random.rand(num_classes, num_features)
bias = np.random.rand(num_classes)
lr = 0.01
num_epochs = 10000

for i in range(num_epochs):
    if i % 100 == 0:
        print(f"Epoch: {i}, Loss: {loss_function(theta, bias, X, y_one_hot)}")
    theta, bias = gradient_descent(theta, bias, X, y_one_hot, lr)

print(theta, bias)
#+end_src

#+RESULTS:
#+begin_example
Epoch: 0, Loss: 1.4383780594596607
Epoch: 100, Loss: 0.8158518365100906
Epoch: 200, Loss: 0.7627475309895695
Epoch: 300, Loss: 0.7205126427693952
Epoch: 400, Loss: 0.6854163643107807
Epoch: 500, Loss: 0.6553837583416365
Epoch: 600, Loss: 0.629166420132039
Epoch: 700, Loss: 0.6059504421192808
Epoch: 800, Loss: 0.5851678883761177
Epoch: 900, Loss: 0.5664012092333598
Epoch: 1000, Loss: 0.549331382646852
Epoch: 1100, Loss: 0.5337077150621502
Epoch: 1200, Loss: 0.5193290486328835
Epoch: 1300, Loss: 0.5060313720541119
Epoch: 1400, Loss: 0.4936792551554217
Epoch: 1500, Loss: 0.48215969808879683
Epoch: 1600, Loss: 0.4713775807293794
Epoch: 1700, Loss: 0.4612522161626732
Epoch: 1800, Loss: 0.45171469138455955
Epoch: 1900, Loss: 0.4427057843652897
Epoch: 2000, Loss: 0.4341743122352627
Epoch: 2100, Loss: 0.4260758076197712
Epoch: 2200, Loss: 0.4183714483604114
Epoch: 2300, Loss: 0.41102718526566706
Epoch: 2400, Loss: 0.40401302622657176
Epoch: 2500, Loss: 0.3973024449065757
Epoch: 2600, Loss: 0.3908718894637273
Epoch: 2700, Loss: 0.38470037216771347
Epoch: 2800, Loss: 0.37876912485652864
Epoch: 2900, Loss: 0.3730613082959608
Epoch: 3000, Loss: 0.3675617659106922
Epoch: 3100, Loss: 0.3622568142277055
Epoch: 3200, Loss: 0.3571340638405527
Epoch: 3300, Loss: 0.35218226586216067
Epoch: 3400, Loss: 0.3473911797549953
Epoch: 3500, Loss: 0.34275145916375915
Epoch: 3600, Loss: 0.33825455296765244
Epoch: 3700, Loss: 0.333892619247353
Epoch: 3800, Loss: 0.32965845024999807
Epoch: 3900, Loss: 0.32554540675196564
Epoch: 4000, Loss: 0.3215473604784451
Epoch: 4100, Loss: 0.31765864345196837
Epoch: 4200, Loss: 0.3138740033180574
Epoch: 4300, Loss: 0.3101885638419918
Epoch: 4400, Loss: 0.3065977898920149
Epoch: 4500, Loss: 0.30309745632554597
Epoch: 4600, Loss: 0.2996836202797567
Epoch: 4700, Loss: 0.2963525964391148
Epoch: 4800, Loss: 0.2931009349125389
Epoch: 4900, Loss: 0.28992540140356454
Epoch: 5000, Loss: 0.2868229593999599
Epoch: 5100, Loss: 0.2837907541458122
Epoch: 5200, Loss: 0.28082609819030596
Epoch: 5300, Loss: 0.27792645833407875
Epoch: 5400, Loss: 0.275089443816889
Epoch: 5500, Loss: 0.27231279560997423
Epoch: 5600, Loss: 0.2695943766933802
Epoch: 5700, Loss: 0.26693216321314456
Epoch: 5800, Loss: 0.264324236425847
Epoch: 5900, Loss: 0.2617687753489963
Epoch: 6000, Loss: 0.25926405004523234
Epoch: 6100, Loss: 0.2568084154766236
Epoch: 6200, Loss: 0.2544003058725607
Epoch: 6300, Loss: 0.2520382295610816
Epoch: 6400, Loss: 0.24972076421899303
Epoch: 6500, Loss: 0.24744655250102404
Epoch: 6600, Loss: 0.2452142980125181
Epoch: 6700, Loss: 0.24302276159393227
Epoch: 6800, Loss: 0.24087075788873963
Epoch: 6900, Loss: 0.23875715216926577
Epoch: 7000, Loss: 0.23668085739758501
Epoch: 7100, Loss: 0.2346408315009125
Epoch: 7200, Loss: 0.23263607484296994
Epoch: 7300, Loss: 0.23066562787462686
Epoch: 7400, Loss: 0.2287285689487348
Epoch: 7500, Loss: 0.22682401228552782
Epoch: 7600, Loss: 0.22495110607624
Epoch: 7700, Loss: 0.2231090307137612
Epoch: 7800, Loss: 0.22129699714017637
Epoch: 7900, Loss: 0.21951424530196517
Epoch: 8000, Loss: 0.2177600427044767
Epoch: 8100, Loss: 0.21603368305803336
Epoch: 8200, Loss: 0.2143344850087054
Epoch: 8300, Loss: 0.21266179094739257
Epoch: 8400, Loss: 0.2110149658914109
Epoch: 8500, Loss: 0.20939339643326688
Epoch: 8600, Loss: 0.20779648975175882
Epoch: 8700, Loss: 0.20622367268094607
Epoch: 8800, Loss: 0.2046743908328962
Epoch: 8900, Loss: 0.2031481077704544
Epoch: 9000, Loss: 0.20164430422658342
Epoch: 9100, Loss: 0.20016247736709775
Epoch: 9200, Loss: 0.19870214009386514
Epoch: 9300, Loss: 0.19726282038578047
Epoch: 9400, Loss: 0.1958440606750243
Epoch: 9500, Loss: 0.19444541725630787
Epoch: 9600, Loss: 0.19306645972698167
Epoch: 9700, Loss: 0.19170677045604356
Epoch: 9800, Loss: 0.1903659440802298
Epoch: 9900, Loss: 0.1890435870254994
[[ 0.04779577 -0.08474396 -0.09984036 -0.07386147]
 [ 0.1807486   0.14857998  0.41066845  0.36575956]
 [ 0.54954818  0.57565603  0.54549412  0.19631824]] [ 5.42783112  0.77522759 -4.99820586]
#+end_example

** C Code
Now let's recreate the same softmax algorithm in C,
following the same steps as the Python implementation.

Functions definition to generate data.
#+name: funCs_data
#+begin_src C :results none :noweb yes :includes <stdio.h> <math.h> 
  // Include libraries 
  #include <stdlib.h>
  #include <time.h>

  // Name: eye
  // Purpose: Create an identity matrix.
  // Return: void
  // Arguments: Number of features and 2D array.
  // Notes: Replication of NumPy "np.eye" function.
  void eye(int num_features, int arr[][num_features]){
    for(int i = 0; i<num_features; i++){
      for(int j = 0; j<num_features; j++){
        arr[i][j] = (i == j) ? 1 : 0;
      }
    }
  }

  // Name: generate_data
  // Purpose: Create gaussian distributed data
  // Return: void
  // Arguments: Number of rows, Number of features
  // Number of classes, Features array X and target y
  void generate_data(int num_rows, int num_features, int num_classes, int X[][num_features], int y[]){
    int examples_per_class = num_rows / num_classes;

    // Further generalizible, example purpose
    // Mean for each class
    int means[3][4] = {
      {2, 2, 2, 2},
      {6, 6, 6, 6},
      {10, 10, 10, 10}
    };

    // seed random generator
    srand(time(0));

    // Generate the data
    for(int class_label = 0; class_label < num_classes; class_label++){
      for(int i = 0; i<examples_per_class; i++){
        int index = class_label * examples_per_class + i;
        for(int j = 0; j<num_features; j++){
          // Generate random data around the mean (Gaussian)
          X[index][j] = means[class_label][j] + (rand() % 3 - 1);
        }
        y[index] = class_label;
      }
    }
  }

  // Name: one_hot_encode
  // Purpose: Create one hot encoded target array
  // Return: void
  // Arguments: Target array y, Number of rows, 
  // Number of classes, Target one hot encoded empty array;
  void one_hot_encode(int y[], int num_rows, int num_classes, int y_one_hot[][num_classes]){
    for(int i = 0; i<num_rows; i++){
      for(int j = 0; j<num_classes; j++){
        y_one_hot[i][j] = (y[i] == j) ? 1 : 0;
      }
    }
  }
#+end_src

Define helper functions for operation with matrices and functions (softmax in this case) computation.
#+name: funC_help
#+begin_src C :results none :noweb yes :includes <stdio.h> <math.h>
  // Name: dot_product
  // Purpose: Compute the dot product of two matrices.
  // Return: void
  // Arguments: Flatten 1D result matrix,
  //            Flatten 1D matrix 1,
  //            Flatten 1D matrix 2,
  //            Number of rows first matrix,
  //            Number of columns first matrix,
  //            Number of columns second matrix.
  // Notes: Replication of NumPy "np.dot" function.  
  void dot_product(double* result, double* matrix1, double* matrix2, int rows1, int cols1, int cols2){
    for(int i = 0; i<rows1; i++){
      for(int j = 0; j<cols2; j++){
        result[i * cols2 + j] = 0.0;
        for(int k = 0; k<cols1; k++){
          result[i * cols2 + j] += matrix1[i * cols1 + k] * matrix2[k * cols2 + j];
        }
      }
    }
  }
  
  // Name: softmax
  // Purpose:  Apply softmax function.
  // Return: void
  // Arguments: Flatten 1D output matrix,
  //            Flatten 1D logits matrix,
  //            Number of rows logits matrix,
  //            Number of columns logits matrix.
  void softmax(double* output, double* logits, int rows, int cols){
    for(int i = 0; i<rows; i++){
      double max_val = logits[i * cols];
      for(int j = 1; j<cols; j++){
        if(logits[i * cols + j] > max_val){
          max_val = logits[i * cols + j];
        }
      }

      double sum_exp = 0.0;
      for(int j = 0; j<cols; j++){
        output[i * cols + j] = exp(logits[i * cols + j] - max_val); // For numerical stability
        sum_exp += output[i * cols + j];
      }

      for(int j = 0; j<cols; j++){
        output[i * cols + j] /=  sum_exp;
      }
    }
  }

  // Name: log_softmax
  // Purpose: Compute element_wise logarithm of softmax probabilities.
  // Return: void
  // Arguments: Flatten 1D output matrix,
  //            Flatten 1D logits matrix,
  //            Number of rows logits matrix,
  //            Number of columns logits matrix.
  void log_softmax(double* output, double* softmax_probs, int rows, int cols){
    for(int i = 0; i<rows*cols; i++){
      output[i] = log(softmax_probs[i]);
    }
  }

  // Name: matrix_subtract
  // Purpose: Subtract two matrices
  // Return: void
  // Arguments: Flatten 1D result matrix,
  //            Flatten 1D matrix 1,
  //            Flatten 1D matrix 2,
  //            Number of rows matrix 1 and 2 (same dimensions),
  //            Number of columns matrix 1 and 2.
  void matrix_subtract(double* result, double* matrix1, double* matrix2, int rows, int cols){
    for(int i = 0; i<rows*cols; i++){
      result[i] = matrix1[i] - matrix2[i];
    }
  }

  // Name: transpose
  // Purpose: Transpose a matrix
  // Return: void
  // Arguments: Flatten 1D result matrix,
  //            Flatten 1D matrix to transpose,
  //            Number of rows of the matrix,
  //            Number of columns of the matrix.
  void transpose(double* result, double* matrix, int rows, int cols){
    for(int i = 0; i<rows; i++){
      for(int j = 0; j<cols; j++){
        result[j * rows + i] = matrix[i * cols + j];
      }
    }
  }
#+end_src

Functions definition for Softmax, Cross-entropy loss and Gradient descent.
#+name: funC_soft
#+begin_src C :results none :noweb yes
  <<funC_help>>

  // Name: compute_softmax
  // Purpose: Compute the softmax function.
  // Return: void
  // Arguments: Flatten 1D softmax probabilities matrix,
  //            Flatten 1D parameter theta matrix,
  //            Flatten 1D bias matrix,
  //            Flatten 1D inputs matrix,
  //            Number of rows (samples),
  //            Number of features,
  //            Number of classes.
  void compute_softmax(double* softmax_probs, double* theta, double* bias, double* X, int num_rows, int  num_features, int num_classes){
    double logits[num_rows * num_classes];

    // Compute logits: z = X * theta.T + bias 
    dot_product(logits, X, theta, num_rows, num_features, num_classes);
    for(int i = 0; i<num_rows; i++){
      for(int j = 0; j<num_classes; j++){
        logits[i * num_classes + j] += bias[j];
      }
    }

    // Apply softmax function
    softmax(softmax_probs, logits, num_rows, num_classes);
    } 

  // Name: compute_loss
  // Purpose: Calculate the cross-entropy loss.
  // Return: double loss
  // Arguments: Flatten 1D parameter theta matrix,
  //            Flatten 1D bias matrix,
  //            Flatten 1D inputs matrix,
  //            1D targets vector,
  //            Number of rows (samples),
  //            Number of features,
  //            Number of classes.
  double compute_loss(double* theta, double* bias, double* X, double* Y, int num_rows, int num_features, int num_classes){
    double softmax_probs[num_rows * num_classes];
    compute_softmax(softmax_probs, theta, bias, X, num_rows, num_features, num_classes);

    double log_probs[num_rows * num_classes];
    log_softmax(log_probs, softmax_probs, num_rows, num_classes);

    double loss = 0.0;
    for(int i = 0; i<num_rows*num_classes; i++){
      loss += Y[i] * log_probs[i];
    } 

    return -loss / num_rows;
  }

  // Name: compute_loss
  // Purpose: Compute the Gradient Descent.
  // Return: void
  // Arguments: Flatten 1D parameter theta matrix,
  //            Flatten 1D bias matrix,
  //            Flatten 1D inputs matrix,
  //            1D targets vector,
  //            Number of rows (samples),
  //            Number of features,
  //            Number of classes.
  //            Learning rate.
  void gradient_descent(double* theta, double* bias, double* X, double* Y, int num_rows, int num_features, int num_classes, double lr){
    double softmax_probs[num_rows * num_classes];
    compute_softmax(softmax_probs, theta, bias, X, num_rows, num_features, num_classes);

    // Compute gradients
    double grad_probs[num_rows * num_classes];
    matrix_subtract(grad_probs, softmax_probs, Y, num_rows, num_classes);

    double theta_grad[num_classes * num_features];
    double grad_probs_T[num_features * num_rows];
    transpose(grad_probs_T, grad_probs, num_rows, num_features);

    dot_product(theta_grad, grad_probs_T, X, num_classes, num_rows, num_features);

    double bias_grad[num_classes];
    for(int i = 0; i<num_classes; i++){
      bias_grad[i] = 0.0;
      for(int j = 0; j<num_rows; j++){
        bias_grad[i] += grad_probs[j * num_classes + i];
      }
      bias_grad[i] /= num_rows;
    }

    for(int i = 0; i<num_classes * num_features; i++){
      theta[i] -= lr * theta_grad[i] / num_rows;
    }
    for(int i = 0; i<num_classes; i++){
      bias[i] -= lr * bias_grad[i];
    }
  }
#+end_src

#+name: Main
#+begin_src C :exports none :main no :noweb yes :cmdline -lm :includes <stdio.h> <math.h> :tangle soft.c
  // Include function definitions
  <<funCs_data>>
  <<funC_soft>>
  
  int main(){
    // Dataset properties
    int num_rows = 100;
    int num_features = 4;
    int num_classes = 3;

    // Training parameters
    int num_epochs = 1000;
    int lr = 0.001;

    // Define Arrays
    int X_2D[num_rows][num_features];
    int y[num_rows];
    int y_one_hot_2D[num_rows][num_classes];

    // Flattened Arrays
    double X[num_rows * num_features];
    double y_one_hot[num_rows * num_classes];

    // Parameters to optimize
    double theta[num_features * num_classes];
    double bias[num_classes];

    // Initialize parameters randomly
    srand(time(0));
    for(int i = 0; i<num_features*num_classes; i++){
      theta[i] = ((double)rand() / RAND_MAX) * 0.1;
    }

    for(int i = 0; i<num_classes; i++){
      bias[i] = 0.0;
    }
    
    // Generate data
    generate_data(num_rows, num_features, num_classes, X_2D, y);

    // One-hot-encode the labels
    one_hot_encode(y, num_rows, num_classes, y_one_hot_2D);

    // Flatten 2D arrays
    for(int i = 0; i<num_rows; i++){
      for(int j = 0; j<num_features; j++){
        X[i * num_features + j] = X_2D[i][j];
      }
      for(int j = 0; j<num_classes; j++){
        y_one_hot[i * num_classes + j] = y_one_hot_2D[i][j];
      }
    }

    // Training loop
    for(int epoch = 0; epoch<num_epochs; epoch++){
      if(epoch % 100 == 0){
        double loss = compute_loss(theta, bias, X, y_one_hot, num_rows, num_features, num_classes);
        printf("Epoch: %d,\tLoss: %.4f\n", epoch, loss);
      }

      gradient_descent(theta, bias, X, y_one_hot, num_rows, num_features, num_classes, lr);
    }

    printf("Final theta values:\n");
    for (int i = 0; i < num_features * num_classes; i++) {
      printf("%.4f ", theta[i]);
      if ((i + 1) % num_features == 0) printf("\n");
    }

    printf("Final bias values:\n");
    for (int i = 0; i < num_classes; i++) {
      printf("%.4f ", bias[i]);
    }
    printf("\n");
    

    return 0;
    }


#+end_src

#+RESULTS: Main



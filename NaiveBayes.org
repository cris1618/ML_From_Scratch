#+TITLE: Naive Bayes from Scratch 
#+AUTHOR: Cristian Del Gobbo (pledged)
#+STARTUP: overview hideblocks indent
#+property: header-args:python :python python3 :session *Python* :results output :exports both :noweb yes :tangle yes:

* Introduction
In this notebook, I will implement a Naive Bayes model from scratch 
using Python and C, without relying on any ML related external libraries.
* Algorithm Description
Naive Bayes is a probabilistic algorithm based on Bayes' Theorem, 
often used for classification tasks like text classification and spam 
detection. It assumes that features (words, in this case) are conditionally 
independent, which simplifies computation but still provides strong results 
for many problems.

** Key Concepts
- *Bayes' Theorem:*
\[
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
\]
Where:  
\( P(C|X) \) – Posterior probability (class given data)  
\( P(X|C) \) – Likelihood (data given class)  
\( P(C) \) – Prior (class probability)  
\( P(X) \) – Evidence (data probability)  

- *Simplification:*  
For multiple features \( X = (x_1, x_2, ..., x_n) \), the formula becomes:  
\[
P(C|X) \propto P(C) \cdot \prod_{i=1}^{n} P(x_i|C)
\]
This assumes all features \( x_i \) are independent.

** How It Works
1. *Training Phase:*
   - Calculate *word frequencies* for spam and non-spam messages separately.  
   - Compute *prior probabilities* for spam and non-spam messages.  
   - Apply *Laplace smoothing* to handle unseen words during classification.

2. *Prediction Phase:*  
   - For a given message, compute the probability of it being spam or non-spam by summing the log of word likelihoods:  
\[
\log P(\text{spam}|X) = \log P(\text{spam}) + \sum_{i} \log P(x_i|\text{spam})
\]  
\[
\log P(\text{non-spam}|X) = \log P(\text{non-spam}) + \sum_{i} \log P(x_i|\text{non-spam})
\]
   - Choose the class with the *higher probability*.

** Advantages
- *Fast and efficient,* even with large datasets.  
- *Handles high-dimensional data* well.  
- *Requires minimal training data*.
** Limitations
- Assumes *conditional independence* between features, which may not hold true in all cases.  
- *Zero-frequency problem* – addressed by *Laplace smoothing*.  
- *Sensitive to irrelevant features*.
** Applications
- *Email spam filtering*  
- *Sentiment analysis*  
- *Document classification*  
* Code
** Python Code
First thing first, let's import the data. The dataset I'll 
use in the following code contains a set of SMS messages
with the respective labels: "spam" if the message was classified
as spam, and "non-spam" otherwise.
#+name: data
#+begin_src python :python python3 :results output
  import numpy as np
  import pandas as pd
  import os
  import string
  from collections import defaultdict
  import nltk
  from nltk.corpus import stopwords

  # Importing the data (SMS Spam vs Non-Spam)
  dir = os.getcwdb().decode('utf-8')
  file_path = os.path.join(dir, 'SMSSpamCollection')
  data = pd.read_csv(file_path, sep='\t', header=None, names=["Labels", "Message"])
  data["Labels"] = data["Labels"].replace("ham", "non-spam")

  #print(data.head())
#+end_src

#+RESULTS: data

To analyze the text, we'll have to perform some preprocessing steps.
In particular, I'll lowercase, remove punctuation, remove stopwords 
and split into words the messages in the dataset.
#+name: preprocess
#+begin_src python :python python3 :results output
  <<data>>

  # Load English stop words
  #nltk.download('stopwords')
  stop_words = set(stopwords.words('english'))

  # Function to preprocess text
  def preprocess_text(text):
      # Lowercase
      text = text.lower()
      # Remove Punctuation
      text = text.translate(str.maketrans('','', string.punctuation))
      # Tokenize by splitting on spaces
      tokens = text.split()
      # Remove stop words
      filtered_tokens = [word for word in tokens if word not in stop_words]

      return filtered_tokens

  # Apply preprocessing to the dataset
  data["Processed"] = data["Message"].apply(preprocess_text)

  #print(data.head())
#+end_src

#+RESULTS: preprocess
#+begin_example
[nltk_data] Downloading package stopwords to
[nltk_data]     /home/uycdcdycdgycdydc/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
     Labels  ...                                          Processed
0  non-spam  ...  [go, jurong, point, crazy, available, bugis, n...
1  non-spam  ...                     [ok, lar, joking, wif, u, oni]
2      spam  ...  [free, entry, 2, wkly, comp, win, fa, cup, fin...
3  non-spam  ...      [u, dun, say, early, hor, u, c, already, say]
4  non-spam  ...  [nah, dont, think, goes, usf, lives, around, t...

[5 rows x 3 columns]
#+end_example

Now let's calculate the word frequencies for each word
in the preprocessed text.
#+name: word
#+begin_src python :python python3 :results output
  <<preprocess>>
  # Function to calculate word frequencies
  def calculate_word_frequencies(data):
      spam_word_counts = defaultdict(int)
      non_spam_word_counts = defaultdict(int)

      for i, row in data.iterrows():
          for word in row["Processed"]:
              if row["Labels"] == "spam":
                  spam_word_counts[word] += 1
              else:
                  non_spam_word_counts[word] += 1

      return spam_word_counts, non_spam_word_counts

  # Calculate frequencies
  spam_counts, non_spam_counts = calculate_word_frequencies(data)

  #print("Most common spam words:", sorted(spam_counts.items(), key=lambda x: x[1], reverse=True)[:10])
  #print("Most common non-spam words:", sorted(non_spam_counts.items(), key=lambda x: x[1], reverse=True)[:10])
#+end_src

#+RESULTS: word
: Most common spam words: [('call', 347), ('free', 216), ('2', 173), ('txt', 150), ('u', 147), ('ur', 144), ('mobile', 123), ('text', 120), ('4', 119), ('stop', 115)]
: Most common non-spam words: [('u', 985), ('im', 451), ('2', 309), ('get', 303), ('ltgt', 276), ('ok', 273), ('dont', 265), ('go', 250), ('ur', 246), ('ill', 238)]

Compute the probabilities of spam and non-spam messages.
#+name: prob
#+begin_src python :python python3 :results output
  <<word>>

  total_messages = len(data)
  spam_messages = len(data[data["Labels"] == "spam"])
  non_spam_messages = total_messages - spam_messages

  prior_spam = spam_messages / total_messages
  prior_non_spam = non_spam_messages / total_messages

  #print(f"Prior Spam probability: {prior_spam:.2f}")
  #print(f"Prior Non-Spam probability: {prior_non_spam:.2f}")
#+end_src

#+RESULTS: prob

Implement Naive Bayes Classifier.
#+name: alg
#+begin_src python :python python3 :results output
  <<prob>>

  def predict(message, spam_counts, non_spam_counts, prior_spam, prior_non_spam):
      message = preprocess_text(message)

      # Initialize Log probabilities
      spam_prob = np.log(prior_spam)
      non_spam_prob = np.log(prior_non_spam)

      # Total number of words in each class
      total_spam_words = sum(spam_counts.values())
      total_non_spam_words = sum(non_spam_counts.values())

      for word in message:
          # Laplace smoothing (add 1)
          spam_prob += np.log((spam_counts[word] + 1) / (total_spam_words + len(spam_counts)))
          non_spam_prob += np.log((non_spam_counts[word] + 1) / (total_spam_words + len(non_spam_counts)))

      return "spam" if spam_prob > non_spam_prob else "non-spam"

  test_message = "Congratulations, you have won a free iPhone!"
  #print(f"Prediction for message: {predict(test_message, spam_counts, non_spam_counts, prior_spam, prior_non_spam)}")
#+end_src

#+RESULTS: alg
: Prediction for message: Spam

Evaluate the simple Naive Bayes model.
#+name: eval
#+begin_src python :python python3 :results output
  <<alg>>
  correct = 0

  for i, row in data.iterrows():
      prediction = predict(row["Message"], spam_counts, non_spam_counts, prior_spam, prior_non_spam)
      if prediction == row["Labels"]:
          correct += 1

  accuracy = correct / total_messages
  print(f"Model Accuracy: {accuracy * 100:.2f}%")
#+end_src

#+RESULTS: eval
: Model Accuracy: 98.40%

** C Code
Now let's follow the same steps, but in C.
1) Loading the data.
#+name: data_load
#+begin_src C :results output :main no :noweb yes
  #include <stdio.h>
  #include <stdlib.h>
  #include <string.h>
  #include <ctype.h>
  #include <math.h>

  #define MAX_LINES 5000
  #define MAX_MESSAGE_LENGTH 1000

  typedef struct SMS{
    char label[10];
    char message[MAX_MESSAGE_LENGTH];
  } SMS;

  typedef struct WordCount{
    char word[50];
    int count;
  } WordCount;

  WordCount* spam_words;
  WordCount* non_spam_words;
  int spam_word_count = 0;
  int non_spam_word_count = 0;
  int spam_capacity = 10000;
  int non_spam_capacity = 10000;
  double prior_spam, prior_non_spam;

  // Name: load_data
  // Purpose: Load a text file.
  // Return: int, number of line 
  // Arguments: Filename, Struct to store data.
  int load_data(const char* filename, SMS* data){
    FILE* file = fopen(filename, "r");
    if(file == NULL){
      perror("Error opening file");
      return -1;
    }

    char line[MAX_MESSAGE_LENGTH + 20]; // extra space for label
    int count = 0;

    while(fgets(line, sizeof(line), file)){
      if(count >= MAX_LINES){
        printf("Exceeded maximum lines.\n");
        break;
      }
      char* label = strtok(line, "\t");
      char* message = strtok(NULL, "\n");

      if(label && message){
        strncpy(data[count].label, label, sizeof(data[count].label)-1);
        strncpy(data[count].message, message, sizeof(data[count].message)-1);
        data[count].label[sizeof(data[count].label) - 1] = '\0';
        data[count].message[sizeof(data[count].message) - 1] = '\0';
        count++;
      } else {
        printf("Skipping malformed line: %s\n", line);
      }
    }

    fclose(file);
    return count;
  }
#+end_src

#+RESULTS: data_load

2) Text Preprocessing.
#+name: text_pre
#+begin_src C :results output :main no :noweb yes
  <<data_load>>

    // Name: to_lowercase
    // Purpose: Convert string to lowercase.
    // Return: void
    // Arguments: String.
  void to_lowercase(char* str){
    for(; *str; str++)
      ,*str = tolower(*str);
    }

  // Name: remove_punctuation
  // Purpose: Remove punctuation from a string.
  // Return: void
  // Arguments: String.
  void remove_punctuation(char* str){
    char* src = str, *dst = str;
    while(*src){
      if(!ispunct((unsigned char)*src)){
        ,*dst++ = *src;
      }
      src++;
    }
    ,*dst = '\0';
  }

  // Name: is_stopword
  // Purpose: check if a word is a stopword.
  // Return: int
  // Arguments: Word (String).
  int is_stopword(const char* word){
    const char* stopwords[] = {"the", "to", "and", "i", "a", "is", "of", "in", "for", "on", "you", "it", "that"};
    int num_stopwords = sizeof(stopwords) / sizeof(stopwords[0]);
    for(int i = 0; i<num_stopwords; i++){
      if(strcmp(word, stopwords[i]) == 0)
        return 1;
    }
    return 0;
  }

  // Name: preprocess_message
  // Purpose: Apply all preprocesing functions.
  // Return: void
  // Arguments: message.
  void preprocess_message(char* message){
    to_lowercase(message);
    remove_punctuation(message);

    char temp[MAX_MESSAGE_LENGTH];
    strcpy(temp, message);

    char* word = strtok(temp, " ");
    message[0] = '\0';

    while(word){
      if(!is_stopword(word)){
        strcat(message, word);
        strcat(message, " ");
      }
      word = strtok(NULL, " ");
    }
  }
#+end_src

#+RESULTS: text_pre

3) Calculate Word Frequencies.
#+name: word_fre
#+begin_src C :results output :main no :noweb yes
  <<text_pre>>
    // Name: resize_array
    // Purpose: Resize array when needed.
    // Return: void
    // Arguments: wordcount, capacity.
  void resize_array(WordCount** counts, int* capacity){
    ,*capacity *= 2; // double the size
    ,*counts = realloc(*counts, (*capacity) * sizeof(WordCount));
    if(*counts == NULL){
      perror("Memory reallocation failed");
      exit(EXIT_FAILURE);
    }
    }

  // Name: update_word_count
  // Purpose: update word frequencies.
  // Return: void
  // Arguments: wordcount, size,capacity, word.
  void update_word_count(WordCount** counts, int* size, int* capacity, const char* word){
    for(int i = 0; i<*size; i++){
      if(strcmp((*counts)[i].word, word) == 0){
        (*counts)[i].count++;
        return;
      }
    }

    // Resize if capacity exceeded
    if(*size >= *capacity){
      resize_array(counts, capacity);
    }

    strcpy((*counts)[*size].word, word);
    (*counts)[*size].count = 1;
    (*size)++;
  }

  // Name: calculate_word_frequencies
  // Purpose: Count word frequencies.
  // Return: void
  // Arguments: SMS data, number of messages.
  void calculate_word_frequencies(SMS* data, int total_messages){
    // Allocate memory
    spam_words = malloc(spam_capacity * sizeof(WordCount));
    non_spam_words = malloc(non_spam_capacity * sizeof(WordCount));

    if(spam_words == NULL || non_spam_words == NULL){
      perror("Memory allocation failed");
      exit(EXIT_FAILURE);
    }

    for(int i = 0; i<total_messages; i++){
      char temp[MAX_MESSAGE_LENGTH];
      strcpy(temp, data[i].message);
      char* word = strtok(temp, " ");

      while(word){
        if(strcmp(data[i].label, "spam") == 0){
          update_word_count(&spam_words, &spam_word_count, &spam_capacity, word);
        } else {
          update_word_count(&non_spam_words, &non_spam_word_count, &non_spam_capacity, word);
        }
        word = strtok(NULL, " ");
      }
    }
  }
#+end_src

#+RESULTS: word_fre

4) Compute Prior Probabilities.
#+name: prob_n
#+begin_src C :results output :main no :noweb yes
  <<word_fre>>

    // Name: compute_prior_probabilities
    // Purpose: Compute prior probabilities.
    // Return: void
    // Arguments: data, total meassages.
  void compute_prior_probabilities(SMS* data, int total_messages){
    int spam_count = 0;

    for(int i = 0; i<total_messages; i++){
      if(strcmp(data[i].label, "spam") == 0)
        spam_count++;
    }

    prior_spam = (double)spam_count / total_messages;
    prior_non_spam = 1.0 - prior_spam;
    }
#+end_src

5) Naive Bayes Algorithm and predictions
#+name: naive
#+begin_src C :results output :main no :noweb yes
  <<prob_n>>

    // Name: predict
    // Purpose: Predict spam or non-spam messages.
    // Return: const char
    // Arguments: message.
  const char* predict(char* message){
    preprocess_message(message);

    double spam_prob = log(prior_spam);
    double non_spam_prob = log(prior_non_spam);

    char temp[MAX_MESSAGE_LENGTH];
    strcpy(temp, message);
    char* word = strtok(temp, " ");

    int total_spam = spam_word_count;
    int total_non_spam = non_spam_word_count;

    while(word){
      int spam_count = 1; // Laplace Smoothing
      int non_spam_count = 1;

      for(int i = 0; i<spam_word_count; i++){
        if(strcmp(spam_words[i].word, word) == 0){
          spam_count += spam_words[i].count;
          break;
        }
      }
      for(int i = 0; i<non_spam_word_count; i++){
        if(strcmp(non_spam_words[i].word, word) == 0){
          non_spam_count += non_spam_words[i].count;
          break;
        }
      }

      spam_prob += log((double)spam_count / (total_spam + spam_word_count));
      non_spam_prob += log((double)non_spam_count / (total_non_spam + non_spam_word_count));
      word = strtok(NULL, " ");
    }
    return (spam_prob > non_spam_prob) ? "spam" : "ham";
    } 

#+end_src

#+RESULTS: naive

6) Evaluation
#+name: evaluate
#+begin_src C :results output :main no :noweb yes
  <<naive>>

    // Name: evaluate
    // Purpose: Evaluate accuracy of the model.
    // Return: double
    // Arguments: SMS data, number of total messages.
  double evaluate(SMS* data, int total_messages){
    int correct = 0;

    for(int i = 0; i<total_messages; i++){
      const char* prediction = predict(data[i].message);
      if(strcmp(prediction, data[i].label) == 0)
        correct++;
    }
    return (double)correct / total_messages * 100;
    }


#+end_src

#+RESULTS: evaluate

7) Main function test
#+name: main
#+begin_src C :results output :noweb yes :tangle naive.c
  <<evaluate>>
  int main(){
    SMS data[MAX_LINES];

    // Load the data
    int total_messages = load_data("./SMSSpamCollection", data);
    if(total_messages == -1){
      return 1;
    }
    printf("Loaded %d messages\n", total_messages);

    // Preprocess and compute word frequencies
    calculate_word_frequencies(data, total_messages);
    compute_prior_probabilities(data, total_messages);

    // Evaluate the classifier
    double accuracy = evaluate(data, total_messages);
    printf("Model Accuracy: %.2f%%\n", accuracy);

    free(spam_words);
    free(non_spam_words);
    return 0;
    }

#+end_src

#+RESULTS: main
